C:\Users\jasyan\Anaconda3\python.exe C:/Users/jasyan/project/meachine_learning_yf/main_drumping.py
正/负 6081/200525
32.97566189771419 34.19761499148211
Index(['CAFE20_gender', 'CAFE20_region', 'CAFE20_levels', 'is_festival_user',
       'level_use', 'is_LAST_2YEAR_DD_ACTIVE', 'cafe_tag_is_mop_available',
       'is_merch_user', 'p4week_active', 'is_LAST_1YEAR_DD_ACTIVE',
       'msr_lifestatus', 'IS_SR_KIT_USER', 'member_monetary', 'skr_rate',
       'merch_rate', 'bev_food_rate', 'food_rate', 'p6m_avg_order_amt',
       'DD_red_rate', 'citytier', 'active_index', 'cafe_tag_p6m_food_qty',
       'total_amt', 'DD_rev', 'svc_revenue', 'DDoffer_rec', 'mop_spend',
       'recency', 'food_party_size', 'multi_bev', 'MC_red_rate', 'SR_KIT_NUM',
       'CAFE20_RECENCY_MERCH', 'cafe_tag_p3m_merch_party_size',
       'CAFE20_VISIT_MERCH', 'CAFE20_P1Y_AVG_TRANX_DAY', 'CAFE20_VISIT_APP',
       'CAFE20_AI', 'CAFE20_age', 'CAFE20_RECENCY_APP',
       'CAFE20_RECENCY_bev_food', 'CAFE20_AMT', 'cafe_tag_p3m_food_qty',
       'rank_preference_food', 'p3m_weekday_trans', 'CAFE20_MONTHLY_FREQ',
       'monthly_freq', 'cafe_tag_p6m_merch_party_size',
       'CAFE20_VISIT_bev_food', 'max_DD_rev', 'p6m_trans',
       'cafe_tag_p6m_monthly_freq', 'DD_end_gap', 'DD_launch_gap',
       'd10_p8week_active', 'p6m_amt', 'cafe_tag_p3m_merch_qty',
       'DD_order_num', 'MC_end_gap', 'p2w_amt', 'CAFE20_VISIT_BEV',
       'CAFE20_RECENCY', 'cafe_tag_p3m_monthly_freq', 'cafe_tag_p6m_merch_qty',
       'p3m_weekly_frq', 'total_trans', 'DD_units', 'max_DD_Quantity',
       'MC_rev', 'p6m_weekday_trans', 'MC_launch_gap', 'MC_units',
       'cafe_tag_p3m_vist', 'MCoffer_red', 'CAFE20_VISIT_SRKIT', 'p2w_trans',
       'CAFE20_RECENCY_SRKIT', 'max_MC_rev', 'CAFE20_P1Y_VISITS_DAY',
       'max_MC_Quantity', 'target_is_DD_ACTIVE'],
      dtype='object')
x_train (144624, 80)
-------------------adaboost-------------------------
================训练集================
              precision    recall  f1-score   support

           0       0.98      0.97      0.97    140446
           1       0.19      0.22      0.20      4178

    accuracy                           0.95    144624
   macro avg       0.58      0.60      0.59    144624
weighted avg       0.95      0.95      0.95    144624

[[136558   3888]
 [  3264    914]]
AUC=0.5955408608125082
================测试集================
              precision    recall  f1-score   support

           0       0.98      0.97      0.97     60079
           1       0.20      0.21      0.20      1903

    accuracy                           0.95     61982
   macro avg       0.59      0.59      0.59     61982
weighted avg       0.95      0.95      0.95     61982

[[58400  1679]
 [ 1496   407]]
AUC=0.5929631476552019
===========b_test===================
              precision    recall  f1-score   support

           0       0.98      0.97      0.97     20074
           1       0.18      0.21      0.19       587

    accuracy                           0.95     20661
   macro avg       0.58      0.59      0.58     20661
weighted avg       0.95      0.95      0.95     20661

[[19512   562]
 [  463   124]]
AUC=0.5916235991567147
================Importance================
                         features  importance
4                       level_use       0.167
3                is_festival_user       0.125
47  cafe_tag_p6m_merch_party_size       0.125
44              p3m_weekday_trans       0.083
23                         DD_rev       0.083
27                        recency       0.083
21          cafe_tag_p6m_food_qty       0.083
20                   active_index       0.083
33  cafe_tag_p3m_merch_party_size       0.042
31                     SR_KIT_NUM       0.042
59                        p2w_amt       0.042
41                     CAFE20_AMT       0.042
52                     DD_end_gap       0.000
60               CAFE20_VISIT_BEV       0.000
54              d10_p8week_active       0.000
51      cafe_tag_p6m_monthly_freq       0.000
55                        p6m_amt       0.000
50                      p6m_trans       0.000
56         cafe_tag_p3m_merch_qty       0.000
57                   DD_order_num       0.000
58                     MC_end_gap       0.000
49                     max_DD_rev       0.000
48          CAFE20_VISIT_bev_food       0.000
46                   monthly_freq       0.000
53                  DD_launch_gap       0.000
0                   CAFE20_gender       0.000
61                 CAFE20_RECENCY       0.000
71                       MC_units       0.000
78          CAFE20_P1Y_VISITS_DAY       0.000
77                     max_MC_rev       0.000
-------------------LR-------------------------
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
C:\Users\jasyan\Anaconda3\lib\site-packages\sklearn\svm\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.
  warnings.warn("Liblinear failed to converge, increase "
================训练集================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98    140446
           1       0.23      0.25      0.24      4178

    accuracy                           0.95    144624
   macro avg       0.60      0.61      0.61    144624
weighted avg       0.96      0.95      0.96    144624

[[137045   3401]
 [  3154   1024]]
AUC=0.6104388166489811
================测试集================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     60079
           1       0.24      0.23      0.24      1903

    accuracy                           0.95     61982
   macro avg       0.61      0.61      0.61     61982
weighted avg       0.95      0.95      0.95     61982

[[58634  1445]
 [ 1456   447]]
AUC=0.6054203050236789
===========b_test===================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     20074
           1       0.23      0.24      0.24       587

    accuracy                           0.96     20661
   macro avg       0.61      0.61      0.61     20661
weighted avg       0.96      0.96      0.96     20661

[[19608   466]
 [  446   141]]
AUC=0.6084951607501987
-------------------Rf-------------------------
{'criterion': 'gini', 'max_depth': 6, 'max_features': 8, 'n_estimators': 23}
================训练集================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98    140446
           1       0.25      0.28      0.26      4178

    accuracy                           0.96    144624
   macro avg       0.62      0.63      0.62    144624
weighted avg       0.96      0.96      0.96    144624

[[137032   3414]
 [  3024   1154]]
AUC=0.6259502186179817
================测试集================
              precision    recall  f1-score   support

           0       0.98      0.97      0.98     60079
           1       0.22      0.23      0.23      1903

    accuracy                           0.95     61982
   macro avg       0.60      0.60      0.60     61982
weighted avg       0.95      0.95      0.95     61982

[[58550  1529]
 [ 1462   441]]
AUC=0.6031447672545565
===========b_test===================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     20074
           1       0.23      0.25      0.24       587

    accuracy                           0.95     20661
   macro avg       0.60      0.61      0.61     20661
weighted avg       0.96      0.95      0.96     20661

[[19582   492]
 [  442   145]]
AUC=0.6112547119100555
================Importance================
                         features  importance
41                     CAFE20_AMT       0.117
4                       level_use       0.070
22                      total_amt       0.062
51      cafe_tag_p6m_monthly_freq       0.058
72              cafe_tag_p3m_vist       0.046
50                      p6m_trans       0.039
42          cafe_tag_p3m_food_qty       0.038
21          cafe_tag_p6m_food_qty       0.027
34             CAFE20_VISIT_MERCH       0.027
23                         DD_rev       0.025
57                   DD_order_num       0.023
12                member_monetary       0.023
32           CAFE20_RECENCY_MERCH       0.019
65                    total_trans       0.019
49                     max_DD_rev       0.018
3                is_festival_user       0.018
56         cafe_tag_p3m_merch_qty       0.016
64                 p3m_weekly_frq       0.016
52                     DD_end_gap       0.016
55                        p6m_amt       0.015
66                       DD_units       0.015
71                       MC_units       0.014
14                     merch_rate       0.013
24                    svc_revenue       0.012
67                max_DD_Quantity       0.011
62      cafe_tag_p3m_monthly_freq       0.011
59                        p2w_amt       0.011
47  cafe_tag_p6m_merch_party_size       0.011
7                   is_merch_user       0.011
27                        recency       0.010
-------------------GBDT-------------------------
{'criterion': 'friedman_mse', 'learning_rate': 0.1, 'loss': 'deviance', 'max_depth': 5, 'max_features': 9, 'n_estimators': 21, 'subsample': 0.8, 'warm_start': True}
================训练集================
              precision    recall  f1-score   support

           0       0.98      0.97      0.98    140446
           1       0.25      0.31      0.28      4178

    accuracy                           0.95    144624
   macro avg       0.62      0.64      0.63    144624
weighted avg       0.96      0.95      0.96    144624

[[136683   3763]
 [  2903   1275]]
AUC=0.6391883609356712
================测试集================
              precision    recall  f1-score   support

           0       0.98      0.97      0.97     60079
           1       0.22      0.25      0.24      1903

    accuracy                           0.95     61982
   macro avg       0.60      0.61      0.61     61982
weighted avg       0.95      0.95      0.95     61982

[[58402  1677]
 [ 1419   484]]
AUC=0.6132110062791121
===========b_test===================
              precision    recall  f1-score   support

           0       0.98      0.97      0.98     20074
           1       0.23      0.27      0.25       587

    accuracy                           0.95     20661
   macro avg       0.60      0.62      0.61     20661
weighted avg       0.96      0.95      0.96     20661

[[19531   543]
 [  428   159]]
AUC=0.6219094546090876
================Importance================
                         features  importance
22                      total_amt       0.104
21          cafe_tag_p6m_food_qty       0.089
27                        recency       0.056
12                member_monetary       0.046
51      cafe_tag_p6m_monthly_freq       0.038
4                       level_use       0.032
23                         DD_rev       0.028
55                        p6m_amt       0.027
20                   active_index       0.026
46                   monthly_freq       0.025
33  cafe_tag_p3m_merch_party_size       0.024
7                   is_merch_user       0.020
52                     DD_end_gap       0.020
67                max_DD_Quantity       0.020
24                    svc_revenue       0.019
34             CAFE20_VISIT_MERCH       0.018
42          cafe_tag_p3m_food_qty       0.017
14                     merch_rate       0.015
57                   DD_order_num       0.015
3                is_festival_user       0.015
59                        p2w_amt       0.015
44              p3m_weekday_trans       0.015
71                       MC_units       0.015
47  cafe_tag_p6m_merch_party_size       0.014
63         cafe_tag_p6m_merch_qty       0.014
68                         MC_rev       0.014
31                     SR_KIT_NUM       0.013
5         is_LAST_2YEAR_DD_ACTIVE       0.013
79                max_MC_Quantity       0.013
41                     CAFE20_AMT       0.012
-------------------XGBOOST-------------------------
{'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 6, 'n_estimators': 24, 'scale_pos_weight': 8.61560555289612, 'subsample': 0.85}
================训练集================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98    140446
           1       0.28      0.30      0.29      4178

    accuracy                           0.96    144624
   macro avg       0.63      0.64      0.63    144624
weighted avg       0.96      0.96      0.96    144624

[[137201   3245]
 [  2930   1248]]
AUC=0.6378012749740625
================测试集==============
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     60079
           1       0.24      0.24      0.24      1903

    accuracy                           0.95     61982
   macro avg       0.61      0.61      0.61     61982
weighted avg       0.95      0.95      0.95     61982

[[58598  1481]
 [ 1445   458]]
AUC=0.6080108729146841
===========b_test===================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     20074
           1       0.23      0.26      0.24       587

    accuracy                           0.96     20661
   macro avg       0.61      0.62      0.61     20661
weighted avg       0.96      0.96      0.96     20661

[[19583   491]
 [  437   150]]
AUC=0.6155385635329859
================Importance================
                         features  importance
4                       level_use       0.137
41                     CAFE20_AMT       0.132
44              p3m_weekday_trans       0.098
3                is_festival_user       0.060
21          cafe_tag_p6m_food_qty       0.034
49                     max_DD_rev       0.033
33  cafe_tag_p3m_merch_party_size       0.024
61                 CAFE20_RECENCY       0.023
20                   active_index       0.023
23                         DD_rev       0.021
22                      total_amt       0.021
11                 IS_SR_KIT_USER       0.019
34             CAFE20_VISIT_MERCH       0.018
62      cafe_tag_p3m_monthly_freq       0.017
31                     SR_KIT_NUM       0.016
42          cafe_tag_p3m_food_qty       0.015
24                    svc_revenue       0.015
27                        recency       0.013
36               CAFE20_VISIT_APP       0.013
14                     merch_rate       0.012
68                         MC_rev       0.012
13                       skr_rate       0.012
59                        p2w_amt       0.011
6       cafe_tag_is_mop_available       0.011
5         is_LAST_2YEAR_DD_ACTIVE       0.011
58                     MC_end_gap       0.011
63         cafe_tag_p6m_merch_qty       0.011
1                   CAFE20_region       0.010
30                    MC_red_rate       0.010
52                     DD_end_gap       0.009
==========LGB===========
[LightGBM] [Info] Total Bins 1303
[LightGBM] [Info] Number of data points in the train set: 144624, number of used features: 80
[LightGBM] [Info] Start training from score 0.288276
[10]	training's l1: 0.401791	training's l2: 0.196994
[20]	training's l1: 0.393913	training's l2: 0.190093
[30]	training's l1: 0.386766	training's l2: 0.184349
[40]	training's l1: 0.380211	training's l2: 0.179465
[50]	training's l1: 0.374157	training's l2: 0.1753
[60]	training's l1: 0.368545	training's l2: 0.171709
[70]	training's l1: 0.363357	training's l2: 0.16862
[80]	training's l1: 0.358596	training's l2: 0.165967
[90]	training's l1: 0.354151	training's l2: 0.163638
[100]	training's l1: 0.350032	training's l2: 0.161606
================训练集================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98    140446
           1       0.26      0.28      0.27      4178

    accuracy                           0.96    144624
   macro avg       0.62      0.63      0.62    144624
weighted avg       0.96      0.96      0.96    144624

[[137106   3340]
 [  3026   1152]]
AUC=0.6259743160963513
================测试集================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     60079
           1       0.23      0.23      0.23      1903

    accuracy                           0.95     61982
   macro avg       0.60      0.60      0.60     61982
weighted avg       0.95      0.95      0.95     61982

[[58612  1467]
 [ 1460   443]]
AUC=0.6041862406125856
===========b_test===================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     20074
           1       0.24      0.25      0.24       587

    accuracy                           0.96     20661
   macro avg       0.61      0.61      0.61     20661
weighted avg       0.96      0.96      0.96     20661

[[19600   474]
 [  440   147]]
AUC=0.6134066305606224
================Importance================
                         features  importance
4                       level_use         171
24                    svc_revenue         159
21          cafe_tag_p6m_food_qty         141
23                         DD_rev         139
14                     merch_rate         111
5         is_LAST_2YEAR_DD_ACTIVE         106
3                is_festival_user         101
39             CAFE20_RECENCY_APP          95
58                     MC_end_gap          94
11                 IS_SR_KIT_USER          89
33  cafe_tag_p3m_merch_party_size          85
68                         MC_rev          85
13                       skr_rate          85
6       cafe_tag_is_mop_available          84
28                food_party_size          82
20                   active_index          82
34             CAFE20_VISIT_MERCH          75
1                   CAFE20_region          75
52                     DD_end_gap          63
36               CAFE20_VISIT_APP          58
49                     max_DD_rev          54
19                       citytier          50
41                     CAFE20_AMT          43
44              p3m_weekday_trans          42
31                     SR_KIT_NUM          41
63         cafe_tag_p6m_merch_qty          41
75                      p2w_trans          40
61                 CAFE20_RECENCY          38
43           rank_preference_food          36
59                        p2w_amt          36
-------------------CATBOOST-------------------------
0:	learn: 0.6354743	total: 1.88s	remaining: 37.6s
0:	learn: 0.6350861	total: 1.5s	remaining: 30s
1:	learn: 0.5903216	total: 2.42s	remaining: 23s
0:	learn: 0.6349182	total: 1.12s	remaining: 22.4s
0:	learn: 0.6349269	total: 856ms	remaining: 18s
1:	learn: 0.5897443	total: 2.14s	remaining: 20.4s
2:	learn: 0.5578499	total: 2.85s	remaining: 17.1s
0:	learn: 0.6362091	total: 1.18s	remaining: 23.5s
0:	learn: 0.6354743	total: 1.26s	remaining: 26.4s
0:	learn: 0.6349269	total: 1.31s	remaining: 26.3s
0:	learn: 0.6349182	total: 1.3s	remaining: 27.3s
2:	learn: 0.5551539	total: 2.69s	remaining: 16.2s
3:	learn: 0.5303904	total: 3.35s	remaining: 14.3s
1:	learn: 0.5895887	total: 1.58s	remaining: 15.8s
1:	learn: 0.5897900	total: 1.89s	remaining: 18s
1:	learn: 0.5933702	total: 1.73s	remaining: 16.4s
3:	learn: 0.5280100	total: 3.2s	remaining: 13.6s
1:	learn: 0.5903216	total: 1.92s	remaining: 19.2s
1:	learn: 0.5897900	total: 1.95s	remaining: 19.5s
1:	learn: 0.5895887	total: 2.07s	remaining: 19.7s
4:	learn: 0.5076441	total: 3.95s	remaining: 12.6s
2:	learn: 0.5552287	total: 2.13s	remaining: 13.5s
2:	learn: 0.5560443	total: 2.54s	remaining: 15.3s
4:	learn: 0.5054091	total: 3.77s	remaining: 12.1s
2:	learn: 0.5574475	total: 2.62s	remaining: 15.7s
2:	learn: 0.5578499	total: 2.66s	remaining: 16.9s
2:	learn: 0.5552287	total: 2.75s	remaining: 16.5s
2:	learn: 0.5560443	total: 2.69s	remaining: 17.1s
3:	learn: 0.5270124	total: 2.82s	remaining: 12.7s
5:	learn: 0.4908122	total: 4.69s	remaining: 11.7s
3:	learn: 0.5279138	total: 3.31s	remaining: 14.1s
3:	learn: 0.5270124	total: 3.25s	remaining: 13.8s
5:	learn: 0.4886699	total: 4.53s	remaining: 11.3s
3:	learn: 0.5303904	total: 3.3s	remaining: 14.9s
4:	learn: 0.5051331	total: 3.37s	remaining: 11.4s
3:	learn: 0.5286400	total: 3.43s	remaining: 14.6s
3:	learn: 0.5279138	total: 3.29s	remaining: 14.8s
6:	learn: 0.4764675	total: 5.27s	remaining: 10.5s
4:	learn: 0.5060470	total: 3.9s	remaining: 12.5s
4:	learn: 0.5051331	total: 3.68s	remaining: 11.8s
5:	learn: 0.4876104	total: 3.73s	remaining: 9.96s
6:	learn: 0.4744693	total: 5.12s	remaining: 10.2s
4:	learn: 0.5076441	total: 3.9s	remaining: 13.3s
4:	learn: 0.5060771	total: 4.08s	remaining: 13.1s
4:	learn: 0.5060470	total: 3.98s	remaining: 13.5s
5:	learn: 0.4876104	total: 4.17s	remaining: 10.4s
7:	learn: 0.4648553	total: 6.03s	remaining: 9.79s
6:	learn: 0.4738684	total: 4.35s	remaining: 9.32s
5:	learn: 0.4890448	total: 4.77s	remaining: 11.9s
7:	learn: 0.4632459	total: 5.8s	remaining: 9.42s
5:	learn: 0.4908122	total: 4.57s	remaining: 12.2s
6:	learn: 0.4738684	total: 4.62s	remaining: 9.24s
5:	learn: 0.4903378	total: 4.75s	remaining: 11.9s
5:	learn: 0.4890448	total: 4.64s	remaining: 12.4s
8:	learn: 0.4558276	total: 6.67s	remaining: 8.89s
7:	learn: 0.4632548	total: 5.02s	remaining: 8.79s
6:	learn: 0.4764675	total: 5.03s	remaining: 10.8s
6:	learn: 0.4750101	total: 5.34s	remaining: 10.7s
8:	learn: 0.4547760	total: 6.54s	remaining: 8.72s
7:	learn: 0.4632548	total: 5.27s	remaining: 8.57s
9:	learn: 0.4490730	total: 7.17s	remaining: 7.88s
6:	learn: 0.4750101	total: 5.25s	remaining: 11.2s
6:	learn: 0.4766219	total: 5.44s	remaining: 10.9s
7:	learn: 0.4648553	total: 5.55s	remaining: 9.72s
8:	learn: 0.4536245	total: 5.61s	remaining: 8.1s
7:	learn: 0.4643838	total: 5.96s	remaining: 9.68s
10:	learn: 0.4431829	total: 7.66s	remaining: 6.96s
8:	learn: 0.4536245	total: 5.82s	remaining: 7.76s
7:	learn: 0.4643838	total: 5.83s	remaining: 10.2s
9:	learn: 0.4473919	total: 7.24s	remaining: 7.96s
7:	learn: 0.4654103	total: 6.05s	remaining: 9.83s
8:	learn: 0.4558276	total: 6.04s	remaining: 8.73s
8:	learn: 0.4553748	total: 6.48s	remaining: 8.64s
9:	learn: 0.4462695	total: 6.25s	remaining: 7.5s
11:	learn: 0.4380464	total: 8.23s	remaining: 6.17s
8:	learn: 0.4558339	total: 6.59s	remaining: 8.79s
9:	learn: 0.4462695	total: 6.54s	remaining: 7.2s
10:	learn: 0.4414331	total: 7.83s	remaining: 7.12s
9:	learn: 0.4490730	total: 6.57s	remaining: 7.88s
8:	learn: 0.4553748	total: 6.6s	remaining: 9.53s
10:	learn: 0.4403313	total: 6.84s	remaining: 6.84s
9:	learn: 0.4484533	total: 7.22s	remaining: 7.94s
12:	learn: 0.4332529	total: 8.82s	remaining: 5.43s
10:	learn: 0.4403313	total: 7.08s	remaining: 6.44s
9:	learn: 0.4483632	total: 7.15s	remaining: 7.86s
11:	learn: 0.4367194	total: 8.39s	remaining: 6.29s
11:	learn: 0.4345936	total: 7.23s	remaining: 6.02s
10:	learn: 0.4431829	total: 7.31s	remaining: 7.31s
9:	learn: 0.4484533	total: 7.24s	remaining: 8.68s
10:	learn: 0.4420052	total: 7.78s	remaining: 7.07s
13:	learn: 0.4293018	total: 9.51s	remaining: 4.75s
12:	learn: 0.4306984	total: 7.7s	remaining: 5.33s
11:	learn: 0.4345936	total: 7.78s	remaining: 5.83s
12:	learn: 0.4319227	total: 9.14s	remaining: 5.63s
11:	learn: 0.4380464	total: 8.01s	remaining: 6.67s
10:	learn: 0.4420052	total: 7.95s	remaining: 7.95s
10:	learn: 0.4420000	total: 8.12s	remaining: 7.38s
11:	learn: 0.4366557	total: 8.4s	remaining: 6.3s
13:	learn: 0.4271617	total: 8.14s	remaining: 4.65s
14:	learn: 0.4263885	total: 10.1s	remaining: 4.05s
12:	learn: 0.4306984	total: 8.52s	remaining: 5.25s
13:	learn: 0.4279121	total: 9.81s	remaining: 4.91s
14:	learn: 0.4241692	total: 8.61s	remaining: 4.02s
12:	learn: 0.4329281	total: 8.97s	remaining: 5.52s
11:	learn: 0.4366557	total: 8.59s	remaining: 7.16s
11:	learn: 0.4373887	total: 8.78s	remaining: 6.58s
12:	learn: 0.4332529	total: 8.8s	remaining: 6.09s
14:	learn: 0.4249766	total: 10.3s	remaining: 4.11s
15:	learn: 0.4243061	total: 10.9s	remaining: 3.41s
15:	learn: 0.4215970	total: 9.1s	remaining: 3.41s
13:	learn: 0.4271617	total: 9.25s	remaining: 4.62s
13:	learn: 0.4290526	total: 9.59s	remaining: 4.79s
12:	learn: 0.4334028	total: 9.4s	remaining: 5.78s
12:	learn: 0.4329281	total: 9.31s	remaining: 6.45s
13:	learn: 0.4293018	total: 9.41s	remaining: 5.38s
16:	learn: 0.4221163	total: 11.4s	remaining: 2.68s
15:	learn: 0.4232341	total: 10.8s	remaining: 3.39s
16:	learn: 0.4194193	total: 9.62s	remaining: 2.83s
14:	learn: 0.4241692	total: 9.72s	remaining: 3.89s
14:	learn: 0.4260733	total: 10.1s	remaining: 4.03s
13:	learn: 0.4298986	total: 10s	remaining: 5.02s
13:	learn: 0.4290526	total: 10s	remaining: 5.73s
14:	learn: 0.4263885	total: 10.1s	remaining: 4.73s
17:	learn: 0.4171881	total: 10.2s	remaining: 2.26s
16:	learn: 0.4209784	total: 11.5s	remaining: 2.7s
15:	learn: 0.4215970	total: 10.4s	remaining: 3.25s
17:	learn: 0.4196890	total: 12.3s	remaining: 2.05s
15:	learn: 0.4236224	total: 10.7s	remaining: 3.35s
14:	learn: 0.4271285	total: 10.6s	remaining: 4.26s
18:	learn: 0.4152416	total: 10.6s	remaining: 1.68s
17:	learn: 0.4187964	total: 12.3s	remaining: 2.05s
15:	learn: 0.4243061	total: 11s	remaining: 4.13s
14:	learn: 0.4260733	total: 11s	remaining: 5.12s
18:	learn: 0.4182882	total: 12.9s	remaining: 1.36s
19:	learn: 0.4135669	total: 11.1s	remaining: 1.11s
16:	learn: 0.4194193	total: 11.1s	remaining: 2.62s
16:	learn: 0.4210813	total: 11.7s	remaining: 2.75s
15:	learn: 0.4237797	total: 11.6s	remaining: 3.63s
15:	learn: 0.4236224	total: 11.5s	remaining: 4.32s
16:	learn: 0.4221163	total: 11.6s	remaining: 3.43s
18:	learn: 0.4174243	total: 12.9s	remaining: 1.36s
19:	learn: 0.4164631	total: 13.6s	remaining: 679ms
20:	learn: 0.4119830	total: 11.7s	remaining: 558ms
17:	learn: 0.4171881	total: 11.7s	remaining: 1.95s
17:	learn: 0.4191899	total: 12.3s	remaining: 2.05s
16:	learn: 0.4213988	total: 12.1s	remaining: 2.86s
18:	learn: 0.4152416	total: 12.2s	remaining: 1.28s
19:	learn: 0.4158476	total: 13.5s	remaining: 675ms
20:	learn: 0.4153986	total: 14.1s	remaining: 0us
17:	learn: 0.4196890	total: 12.3s	remaining: 2.72s
16:	learn: 0.4210813	total: 12.2s	remaining: 3.6s
21:	learn: 0.4105615	total: 12.4s	remaining: 0us
19:	learn: 0.4135669	total: 12.6s	remaining: 631ms
18:	learn: 0.4175146	total: 13s	remaining: 1.36s
17:	learn: 0.4189896	total: 12.7s	remaining: 2.12s
17:	learn: 0.4191899	total: 12.7s	remaining: 2.82s
18:	learn: 0.4182882	total: 12.8s	remaining: 2.02s
20:	learn: 0.4144217	total: 14.2s	remaining: 0us
19:	learn: 0.4155658	total: 13.5s	remaining: 676ms
18:	learn: 0.4175146	total: 13.2s	remaining: 2.08s
20:	learn: 0.4119830	total: 13.3s	remaining: 0us
18:	learn: 0.4171230	total: 13.4s	remaining: 1.41s
19:	learn: 0.4164631	total: 13.5s	remaining: 1.35s
19:	learn: 0.4158786	total: 13.7s	remaining: 687ms
19:	learn: 0.4155658	total: 13.6s	remaining: 1.36s
20:	learn: 0.4143638	total: 14.1s	remaining: 0us
20:	learn: 0.4153986	total: 14s	remaining: 669ms
20:	learn: 0.4144944	total: 14.1s	remaining: 0us
20:	learn: 0.4143638	total: 14.2s	remaining: 675ms
21:	learn: 0.4138418	total: 14.6s	remaining: 0us
21:	learn: 0.4130243	total: 14.7s	remaining: 0us
0:	learn: 0.6350861	total: 651ms	remaining: 13.7s
0:	learn: 0.6362091	total: 989ms	remaining: 20.8s
1:	learn: 0.5897443	total: 1.62s	remaining: 16.2s
1:	learn: 0.5933702	total: 1.82s	remaining: 18.2s
0:	learn: 0.6354743	total: 1.14s	remaining: 25.1s
0:	learn: 0.6349269	total: 1.11s	remaining: 24.3s
2:	learn: 0.5551539	total: 2.57s	remaining: 16.3s
2:	learn: 0.5574475	total: 2.6s	remaining: 16.4s
1:	learn: 0.5903216	total: 1.94s	remaining: 20.4s
1:	learn: 0.5895887	total: 1.82s	remaining: 19.1s
0:	learn: 0.6349182	total: 798ms	remaining: 17.6s
3:	learn: 0.5286400	total: 3.08s	remaining: 13.8s
0:	learn: 0.6350861	total: 913ms	remaining: 20.1s
3:	learn: 0.5280100	total: 3.49s	remaining: 15.7s
2:	learn: 0.5578499	total: 2.82s	remaining: 18.8s
0:	learn: 0.6362091	total: 870ms	remaining: 19.1s
4:	learn: 0.5060771	total: 3.67s	remaining: 12.5s
2:	learn: 0.5552287	total: 2.62s	remaining: 17.5s
1:	learn: 0.5897900	total: 1.71s	remaining: 17.9s
5:	learn: 0.4903378	total: 4.09s	remaining: 10.9s
4:	learn: 0.5054091	total: 4.18s	remaining: 14.2s
1:	learn: 0.5897443	total: 1.73s	remaining: 18.2s
3:	learn: 0.5303904	total: 3.39s	remaining: 16.1s
1:	learn: 0.5933702	total: 1.49s	remaining: 15.7s
3:	learn: 0.5270124	total: 3.24s	remaining: 15.4s
0:	learn: 0.6353913	total: 667ms	remaining: 13.3s
2:	learn: 0.5560443	total: 2.36s	remaining: 15.8s
6:	learn: 0.4766219	total: 4.51s	remaining: 9.67s
5:	learn: 0.4886699	total: 4.91s	remaining: 13.1s
2:	learn: 0.5551539	total: 2.38s	remaining: 15.9s
4:	learn: 0.5051331	total: 3.79s	remaining: 13.7s
4:	learn: 0.5076441	total: 4.08s	remaining: 14.7s
1:	learn: 0.5885841	total: 1.35s	remaining: 12.8s
2:	learn: 0.5574475	total: 2.33s	remaining: 15.5s
7:	learn: 0.4654103	total: 5.26s	remaining: 9.2s
3:	learn: 0.5279138	total: 3.22s	remaining: 15.3s
3:	learn: 0.5280100	total: 3.04s	remaining: 14.4s
5:	learn: 0.4876104	total: 4.47s	remaining: 12.7s
6:	learn: 0.4744693	total: 5.65s	remaining: 12.1s
5:	learn: 0.4908122	total: 4.79s	remaining: 13.6s
3:	learn: 0.5286400	total: 3.01s	remaining: 14.3s
4:	learn: 0.5060470	total: 3.83s	remaining: 13.8s
2:	learn: 0.5532945	total: 2.27s	remaining: 13.6s
8:	learn: 0.4558339	total: 6s	remaining: 8.67s
6:	learn: 0.4738684	total: 4.95s	remaining: 11.3s
4:	learn: 0.5054091	total: 3.7s	remaining: 13.3s
4:	learn: 0.5060771	total: 3.45s	remaining: 12.4s
6:	learn: 0.4764675	total: 5.47s	remaining: 12.5s
7:	learn: 0.4632459	total: 6.4s	remaining: 11.2s
5:	learn: 0.4890448	total: 4.31s	remaining: 12.2s
9:	learn: 0.4483632	total: 6.52s	remaining: 7.82s
3:	learn: 0.5253273	total: 3.02s	remaining: 12.8s
7:	learn: 0.4632548	total: 5.71s	remaining: 10.7s
5:	learn: 0.4886699	total: 4.43s	remaining: 12.6s
8:	learn: 0.4547760	total: 7.3s	remaining: 10.5s
5:	learn: 0.4903378	total: 4.46s	remaining: 12.6s
7:	learn: 0.4648553	total: 6.43s	remaining: 12s
10:	learn: 0.4420000	total: 7.25s	remaining: 7.25s
6:	learn: 0.4750101	total: 5.25s	remaining: 12s
6:	learn: 0.4744693	total: 5.09s	remaining: 11.6s
8:	learn: 0.4536245	total: 6.63s	remaining: 10.3s
4:	learn: 0.5032553	total: 4.21s	remaining: 13.5s
9:	learn: 0.4473919	total: 8.11s	remaining: 9.73s
11:	learn: 0.4373887	total: 8.08s	remaining: 6.73s
7:	learn: 0.4632459	total: 5.73s	remaining: 10.7s
6:	learn: 0.4766219	total: 5.45s	remaining: 12.5s
7:	learn: 0.4643838	total: 6.21s	remaining: 11.6s
8:	learn: 0.4558276	total: 7.55s	remaining: 11.7s
9:	learn: 0.4462695	total: 7.46s	remaining: 9.7s
12:	learn: 0.4334028	total: 8.67s	remaining: 6s
10:	learn: 0.4414331	total: 8.93s	remaining: 8.93s
8:	learn: 0.4553748	total: 6.87s	remaining: 10.7s
5:	learn: 0.4856793	total: 5.36s	remaining: 13.4s
7:	learn: 0.4654103	total: 6.29s	remaining: 11.8s
10:	learn: 0.4403313	total: 8.05s	remaining: 8.78s
8:	learn: 0.4547760	total: 6.69s	remaining: 10.4s
13:	learn: 0.4298986	total: 9.21s	remaining: 5.26s
9:	learn: 0.4490730	total: 8.39s	remaining: 10.9s
9:	learn: 0.4484533	total: 7.51s	remaining: 9.77s
11:	learn: 0.4367194	total: 9.79s	remaining: 8.16s
14:	learn: 0.4271285	total: 9.71s	remaining: 4.53s
11:	learn: 0.4345936	total: 8.67s	remaining: 7.95s
9:	learn: 0.4473919	total: 7.43s	remaining: 9.66s
8:	learn: 0.4558339	total: 7.17s	remaining: 11.1s
10:	learn: 0.4431829	total: 9.14s	remaining: 9.97s
6:	learn: 0.4712520	total: 6.38s	remaining: 12.8s
12:	learn: 0.4319227	total: 10.4s	remaining: 7.21s
15:	learn: 0.4237797	total: 10.4s	remaining: 3.92s
12:	learn: 0.4306984	total: 9.4s	remaining: 7.23s
10:	learn: 0.4420052	total: 8.4s	remaining: 9.16s
10:	learn: 0.4414331	total: 8.07s	remaining: 8.8s
9:	learn: 0.4483632	total: 7.84s	remaining: 10.2s
11:	learn: 0.4380464	total: 9.88s	remaining: 9.05s
16:	learn: 0.4213988	total: 11.1s	remaining: 3.26s
7:	learn: 0.4605332	total: 7.42s	remaining: 12.1s
11:	learn: 0.4367194	total: 8.63s	remaining: 7.91s
10:	learn: 0.4420000	total: 8.45s	remaining: 9.22s
13:	learn: 0.4279121	total: 11.3s	remaining: 6.48s
13:	learn: 0.4271617	total: 10.2s	remaining: 6.59s
11:	learn: 0.4366557	total: 9.19s	remaining: 8.42s
12:	learn: 0.4332529	total: 10.6s	remaining: 8.12s
12:	learn: 0.4319227	total: 9.1s	remaining: 7s
11:	learn: 0.4373887	total: 9.07s	remaining: 8.32s
17:	learn: 0.4189896	total: 11.9s	remaining: 2.64s
12:	learn: 0.4329281	total: 9.79s	remaining: 7.53s
14:	learn: 0.4249766	total: 12.1s	remaining: 5.64s
13:	learn: 0.4279121	total: 9.49s	remaining: 6.1s
14:	learn: 0.4241692	total: 10.9s	remaining: 5.83s
8:	learn: 0.4517224	total: 8.31s	remaining: 11.1s
13:	learn: 0.4293018	total: 11.3s	remaining: 7.26s
12:	learn: 0.4334028	total: 9.57s	remaining: 7.36s
18:	learn: 0.4171230	total: 12.4s	remaining: 1.96s
14:	learn: 0.4249766	total: 10s	remaining: 5.33s
13:	learn: 0.4290526	total: 10.4s	remaining: 6.67s
15:	learn: 0.4215970	total: 11.5s	remaining: 5.03s
15:	learn: 0.4232341	total: 12.8s	remaining: 4.8s
14:	learn: 0.4263885	total: 11.9s	remaining: 6.37s
13:	learn: 0.4298986	total: 10s	remaining: 6.45s
9:	learn: 0.4442003	total: 9.14s	remaining: 10.1s
15:	learn: 0.4232341	total: 10.4s	remaining: 4.57s
19:	learn: 0.4158786	total: 13s	remaining: 1.3s
14:	learn: 0.4260733	total: 11s	remaining: 5.86s
16:	learn: 0.4194193	total: 12.1s	remaining: 4.28s
14:	learn: 0.4271285	total: 10.5s	remaining: 5.59s
16:	learn: 0.4209784	total: 13.5s	remaining: 3.97s
15:	learn: 0.4243061	total: 12.6s	remaining: 5.53s
16:	learn: 0.4209784	total: 11.1s	remaining: 3.9s
15:	learn: 0.4237797	total: 11s	remaining: 4.81s
20:	learn: 0.4144944	total: 13.8s	remaining: 657ms
10:	learn: 0.4386344	total: 10.1s	remaining: 9.19s
17:	learn: 0.4171881	total: 12.8s	remaining: 3.56s
17:	learn: 0.4187964	total: 14.1s	remaining: 3.14s
15:	learn: 0.4236224	total: 11.9s	remaining: 5.21s
16:	learn: 0.4221163	total: 13.3s	remaining: 4.68s
17:	learn: 0.4187964	total: 11.6s	remaining: 3.22s
21:	learn: 0.4134430	total: 14.3s	remaining: 0us
16:	learn: 0.4213988	total: 11.7s	remaining: 4.12s
18:	learn: 0.4152416	total: 13.4s	remaining: 2.82s
11:	learn: 0.4335373	total: 10.8s	remaining: 8.11s
18:	learn: 0.4174243	total: 14.7s	remaining: 2.32s
18:	learn: 0.4174243	total: 12.2s	remaining: 2.56s
17:	learn: 0.4196890	total: 13.9s	remaining: 3.87s
17:	learn: 0.4189896	total: 12s	remaining: 3.34s
16:	learn: 0.4210813	total: 12.7s	remaining: 4.48s
19:	learn: 0.4135669	total: 14.1s	remaining: 2.11s
19:	learn: 0.4158476	total: 15.3s	remaining: 1.53s
12:	learn: 0.4298241	total: 11.5s	remaining: 7.09s
18:	learn: 0.4171230	total: 12.5s	remaining: 2.63s
19:	learn: 0.4158476	total: 12.8s	remaining: 1.92s
18:	learn: 0.4182882	total: 14.5s	remaining: 3.06s
19:	learn: 0.4158786	total: 12.9s	remaining: 1.94s
13:	learn: 0.4271744	total: 12s	remaining: 6.02s
17:	learn: 0.4191899	total: 13.7s	remaining: 3.81s
20:	learn: 0.4119830	total: 14.9s	remaining: 1.42s
20:	learn: 0.4144217	total: 16.1s	remaining: 767ms
20:	learn: 0.4144944	total: 13.4s	remaining: 1.28s
20:	learn: 0.4144217	total: 13.7s	remaining: 1.3s
14:	learn: 0.4235656	total: 12.6s	remaining: 5.04s
19:	learn: 0.4164631	total: 15.5s	remaining: 2.32s
18:	learn: 0.4175146	total: 14.4s	remaining: 3.03s
21:	learn: 0.4134430	total: 13.9s	remaining: 632ms
21:	learn: 0.4105615	total: 15.6s	remaining: 711ms
21:	learn: 0.4130886	total: 16.9s	remaining: 0us
15:	learn: 0.4207047	total: 13.2s	remaining: 4.12s
21:	learn: 0.4130886	total: 14.5s	remaining: 659ms
20:	learn: 0.4153986	total: 16.2s	remaining: 1.54s
22:	learn: 0.4119779	total: 14.5s	remaining: 0us
19:	learn: 0.4155658	total: 15.1s	remaining: 2.27s
22:	learn: 0.4095143	total: 16.3s	remaining: 0us
16:	learn: 0.4190083	total: 13.8s	remaining: 3.25s
22:	learn: 0.4115314	total: 15s	remaining: 0us
21:	learn: 0.4138418	total: 16.9s	remaining: 767ms
20:	learn: 0.4143638	total: 15.6s	remaining: 1.48s
17:	learn: 0.4162747	total: 14.3s	remaining: 2.39s
22:	learn: 0.4118732	total: 17.3s	remaining: 0us
21:	learn: 0.4130243	total: 16.3s	remaining: 740ms
22:	learn: 0.4118153	total: 16.6s	remaining: 0us
18:	learn: 0.4143716	total: 15.2s	remaining: 1.6s
0:	learn: 0.6336644	total: 842ms	remaining: 16.8s
19:	learn: 0.4129093	total: 15.9s	remaining: 793ms
20:	learn: 0.4114817	total: 16.4s	remaining: 0us
1:	learn: 0.5894176	total: 1.73s	remaining: 16.4s
2:	learn: 0.5543699	total: 2.62s	remaining: 15.7s
0:	learn: 0.6348368	total: 1.05s	remaining: 20.9s
3:	learn: 0.5247380	total: 3.28s	remaining: 13.9s
0:	learn: 0.6358963	total: 753ms	remaining: 15.1s
1:	learn: 0.5897336	total: 1.68s	remaining: 15.9s
0:	learn: 0.6349952	total: 901ms	remaining: 18s
0:	learn: 0.6353913	total: 922ms	remaining: 19.4s
1:	learn: 0.5916722	total: 1.75s	remaining: 16.6s
4:	learn: 0.5021766	total: 4.41s	remaining: 14.1s
1:	learn: 0.5900043	total: 1.89s	remaining: 17.9s
2:	learn: 0.5543638	total: 2.8s	remaining: 16.8s
1:	learn: 0.5885841	total: 1.77s	remaining: 17.7s
2:	learn: 0.5541039	total: 2.4s	remaining: 14.4s
2:	learn: 0.5559819	total: 2.58s	remaining: 15.5s
5:	learn: 0.4844800	total: 5.27s	remaining: 13.2s
0:	learn: 0.6336644	total: 1.07s	remaining: 22.5s
3:	learn: 0.5255720	total: 3.61s	remaining: 15.3s
0:	learn: 0.6348368	total: 809ms	remaining: 17s
2:	learn: 0.5532945	total: 2.68s	remaining: 17s
3:	learn: 0.5261500	total: 3.16s	remaining: 13.5s
4:	learn: 0.5033261	total: 4.23s	remaining: 13.5s
3:	learn: 0.5288846	total: 3.47s	remaining: 14.7s
6:	learn: 0.4696249	total: 6.31s	remaining: 12.6s
1:	learn: 0.5894176	total: 2.1s	remaining: 21s
1:	learn: 0.5897336	total: 1.73s	remaining: 17.3s
4:	learn: 0.5039218	total: 3.95s	remaining: 12.6s
0:	learn: 0.6349952	total: 1.02s	remaining: 21.5s
3:	learn: 0.5253273	total: 3.63s	remaining: 16.3s
4:	learn: 0.5075589	total: 4.16s	remaining: 13.3s
5:	learn: 0.4856543	total: 5s	remaining: 12.5s
7:	learn: 0.4585697	total: 6.85s	remaining: 11.1s
2:	learn: 0.5543699	total: 3.1s	remaining: 19.6s
2:	learn: 0.5543638	total: 2.6s	remaining: 16.5s
8:	learn: 0.4494084	total: 7.57s	remaining: 10.1s
5:	learn: 0.4861833	total: 4.87s	remaining: 12.2s
5:	learn: 0.4894574	total: 5.02s	remaining: 12.5s
6:	learn: 0.4708728	total: 5.91s	remaining: 11.8s
1:	learn: 0.5900043	total: 2.02s	remaining: 20.2s
4:	learn: 0.5032553	total: 4.65s	remaining: 15.8s
9:	learn: 0.4422569	total: 7.95s	remaining: 8.74s
3:	learn: 0.5247380	total: 3.86s	remaining: 17.4s
3:	learn: 0.5255720	total: 3.43s	remaining: 15.4s
6:	learn: 0.4713426	total: 5.63s	remaining: 11.3s
6:	learn: 0.4748154	total: 5.71s	remaining: 11.4s
7:	learn: 0.4600845	total: 6.58s	remaining: 10.7s
10:	learn: 0.4366204	total: 8.44s	remaining: 7.67s
2:	learn: 0.5541039	total: 2.79s	remaining: 17.7s
5:	learn: 0.4856793	total: 5.48s	remaining: 14.6s
11:	learn: 0.4313476	total: 8.96s	remaining: 6.72s
4:	learn: 0.5021766	total: 4.72s	remaining: 16.1s
4:	learn: 0.5033261	total: 4.19s	remaining: 14.3s
7:	learn: 0.4628703	total: 6.35s	remaining: 10.3s
7:	learn: 0.4608459	total: 6.32s	remaining: 10.3s
3:	learn: 0.5261500	total: 3.4s	remaining: 15.3s
8:	learn: 0.4512306	total: 7.36s	remaining: 9.82s
6:	learn: 0.4712520	total: 6.26s	remaining: 13.4s
12:	learn: 0.4276566	total: 9.68s	remaining: 5.96s
8:	learn: 0.4532961	total: 7.06s	remaining: 9.41s
5:	learn: 0.4856543	total: 4.94s	remaining: 13.2s
8:	learn: 0.4515858	total: 7.05s	remaining: 9.4s
4:	learn: 0.5039218	total: 4.05s	remaining: 13.8s
5:	learn: 0.4844800	total: 5.52s	remaining: 14.7s
9:	learn: 0.4434163	total: 8.04s	remaining: 8.84s
7:	learn: 0.4605332	total: 6.86s	remaining: 12s
9:	learn: 0.4436350	total: 7.71s	remaining: 8.48s
13:	learn: 0.4248066	total: 10.4s	remaining: 5.21s
6:	learn: 0.4708728	total: 5.64s	remaining: 12.1s
5:	learn: 0.4861833	total: 4.79s	remaining: 12.8s
9:	learn: 0.4461379	total: 7.85s	remaining: 8.63s
10:	learn: 0.4378136	total: 8.71s	remaining: 7.92s
6:	learn: 0.4696249	total: 6.28s	remaining: 13.5s
8:	learn: 0.4517224	total: 7.54s	remaining: 10.9s
10:	learn: 0.4381139	total: 8.13s	remaining: 7.39s
7:	learn: 0.4600845	total: 6.36s	remaining: 11.1s
10:	learn: 0.4395294	total: 8.58s	remaining: 7.8s
6:	learn: 0.4713426	total: 5.58s	remaining: 11.9s
14:	learn: 0.4221737	total: 11.3s	remaining: 4.52s
11:	learn: 0.4327181	total: 9.53s	remaining: 7.15s
11:	learn: 0.4330251	total: 8.66s	remaining: 6.5s
9:	learn: 0.4442003	total: 8.21s	remaining: 9.85s
7:	learn: 0.4585697	total: 7.18s	remaining: 12.6s
10:	learn: 0.4386344	total: 8.7s	remaining: 8.7s
12:	learn: 0.4290242	total: 9.23s	remaining: 5.68s
8:	learn: 0.4512306	total: 7.14s	remaining: 10.3s
7:	learn: 0.4608459	total: 6.29s	remaining: 11s
15:	learn: 0.4193868	total: 12.1s	remaining: 3.77s
12:	learn: 0.4289656	total: 10.3s	remaining: 6.34s
11:	learn: 0.4340045	total: 9.47s	remaining: 7.1s
8:	learn: 0.4494084	total: 8.08s	remaining: 11.7s
13:	learn: 0.4256841	total: 9.86s	remaining: 4.93s
11:	learn: 0.4335373	total: 9.44s	remaining: 7.86s
9:	learn: 0.4434163	total: 7.88s	remaining: 9.46s
16:	learn: 0.4175573	total: 12.7s	remaining: 3s
8:	learn: 0.4515858	total: 7.06s	remaining: 10.2s
12:	learn: 0.4293924	total: 10.2s	remaining: 6.29s
13:	learn: 0.4257749	total: 11.1s	remaining: 5.56s
14:	learn: 0.4227763	total: 10.3s	remaining: 4.14s
9:	learn: 0.4422569	total: 8.94s	remaining: 10.7s
12:	learn: 0.4298241	total: 10.2s	remaining: 7.09s
17:	learn: 0.4150931	total: 13.6s	remaining: 2.26s
9:	learn: 0.4436350	total: 7.92s	remaining: 9.5s
10:	learn: 0.4378136	total: 8.84s	remaining: 8.84s
14:	learn: 0.4230028	total: 11.9s	remaining: 4.75s
15:	learn: 0.4202550	total: 11s	remaining: 3.43s
13:	learn: 0.4253545	total: 11.1s	remaining: 5.54s
10:	learn: 0.4366204	total: 9.7s	remaining: 9.7s
13:	learn: 0.4271744	total: 11.1s	remaining: 6.35s
16:	learn: 0.4183459	total: 11.7s	remaining: 2.75s
10:	learn: 0.4381139	total: 8.73s	remaining: 8.73s
11:	learn: 0.4327181	total: 9.7s	remaining: 8.08s
18:	learn: 0.4128117	total: 14.5s	remaining: 1.53s
15:	learn: 0.4202665	total: 12.7s	remaining: 3.97s
14:	learn: 0.4221084	total: 11.9s	remaining: 4.77s
11:	learn: 0.4313476	total: 10.4s	remaining: 8.71s
14:	learn: 0.4235656	total: 11.8s	remaining: 5.49s
11:	learn: 0.4330251	total: 9.46s	remaining: 7.88s
17:	learn: 0.4158966	total: 12.5s	remaining: 2.08s
12:	learn: 0.4289656	total: 10.4s	remaining: 7.17s
15:	learn: 0.4198154	total: 12.6s	remaining: 3.95s
19:	learn: 0.4113793	total: 15.3s	remaining: 765ms
12:	learn: 0.4276566	total: 11.1s	remaining: 7.67s
16:	learn: 0.4185146	total: 13.6s	remaining: 3.19s
15:	learn: 0.4207047	total: 12.5s	remaining: 4.67s
18:	learn: 0.4138350	total: 13.1s	remaining: 1.38s
16:	learn: 0.4173313	total: 13.2s	remaining: 3.1s
12:	learn: 0.4290242	total: 10.2s	remaining: 7.09s
13:	learn: 0.4257749	total: 11.1s	remaining: 6.36s
13:	learn: 0.4248066	total: 11.7s	remaining: 6.71s
20:	learn: 0.4092647	total: 16s	remaining: 0us
17:	learn: 0.4164008	total: 14.2s	remaining: 2.37s
16:	learn: 0.4190083	total: 13.1s	remaining: 3.86s
19:	learn: 0.4122843	total: 13.9s	remaining: 697ms
17:	learn: 0.4149399	total: 14s	remaining: 2.34s
14:	learn: 0.4221737	total: 12.4s	remaining: 5.81s
14:	learn: 0.4230028	total: 11.9s	remaining: 5.57s
18:	learn: 0.4141910	total: 15s	remaining: 1.58s
17:	learn: 0.4162747	total: 13.7s	remaining: 3.05s
13:	learn: 0.4256841	total: 11.2s	remaining: 6.4s
18:	learn: 0.4127668	total: 14.7s	remaining: 1.54s
20:	learn: 0.4107097	total: 14.8s	remaining: 0us
18:	learn: 0.4143716	total: 14.3s	remaining: 2.26s
15:	learn: 0.4202665	total: 12.7s	remaining: 4.77s
19:	learn: 0.4127386	total: 15.8s	remaining: 791ms
15:	learn: 0.4193868	total: 13.4s	remaining: 5.02s
14:	learn: 0.4227763	total: 12s	remaining: 5.59s
19:	learn: 0.4113409	total: 15.1s	remaining: 757ms
20:	learn: 0.4112902	total: 16.3s	remaining: 0us
19:	learn: 0.4129093	total: 15s	remaining: 1.5s
20:	learn: 0.4096663	total: 15.6s	remaining: 0us
16:	learn: 0.4175573	total: 14.1s	remaining: 4.14s
16:	learn: 0.4185146	total: 13.6s	remaining: 3.99s
15:	learn: 0.4202550	total: 12.8s	remaining: 4.8s
20:	learn: 0.4114817	total: 15.6s	remaining: 742ms
17:	learn: 0.4150931	total: 14.6s	remaining: 3.24s
16:	learn: 0.4183459	total: 13.2s	remaining: 3.9s
17:	learn: 0.4164008	total: 14.1s	remaining: 3.14s
21:	learn: 0.4104796	total: 16s	remaining: 0us
18:	learn: 0.4128117	total: 15.1s	remaining: 2.38s
17:	learn: 0.4158966	total: 13.8s	remaining: 3.06s
18:	learn: 0.4141910	total: 14.8s	remaining: 2.34s
19:	learn: 0.4113793	total: 15.5s	remaining: 1.55s
18:	learn: 0.4138350	total: 14.4s	remaining: 2.27s
19:	learn: 0.4127386	total: 15.3s	remaining: 1.53s
20:	learn: 0.4092647	total: 16.1s	remaining: 768ms
19:	learn: 0.4122843	total: 15s	remaining: 1.5s
20:	learn: 0.4112902	total: 16s	remaining: 761ms
21:	learn: 0.4073815	total: 16.9s	remaining: 0us
0:	learn: 0.6358963	total: 836ms	remaining: 17.6s
20:	learn: 0.4107097	total: 16s	remaining: 764ms
21:	learn: 0.4102003	total: 17s	remaining: 0us
0:	learn: 0.6353913	total: 716ms	remaining: 15.7s
1:	learn: 0.5916722	total: 1.64s	remaining: 16.4s
21:	learn: 0.4090258	total: 16.9s	remaining: 0us
0:	learn: 0.6336644	total: 948ms	remaining: 20.8s
0:	learn: 0.6348368	total: 914ms	remaining: 20.1s
1:	learn: 0.5885841	total: 1.63s	remaining: 17.1s
2:	learn: 0.5559819	total: 2.48s	remaining: 15.7s
1:	learn: 0.5897336	total: 1.33s	remaining: 14s
1:	learn: 0.5894176	total: 1.73s	remaining: 18.1s
2:	learn: 0.5532945	total: 2.36s	remaining: 15.7s
2:	learn: 0.5543638	total: 1.82s	remaining: 12.1s
0:	learn: 0.6349952	total: 717ms	remaining: 15.8s
3:	learn: 0.5288846	total: 3.17s	remaining: 14.3s
2:	learn: 0.5543699	total: 2.3s	remaining: 15.3s
3:	learn: 0.5255720	total: 2.35s	remaining: 11.2s
3:	learn: 0.5253273	total: 3.02s	remaining: 14.3s
1:	learn: 0.5900043	total: 1.46s	remaining: 15.3s
4:	learn: 0.5075589	total: 3.87s	remaining: 13.2s
3:	learn: 0.5247380	total: 2.84s	remaining: 13.5s
4:	learn: 0.5033261	total: 2.89s	remaining: 10.4s
4:	learn: 0.5032553	total: 3.67s	remaining: 13.2s
2:	learn: 0.5541039	total: 2.18s	remaining: 14.5s
4:	learn: 0.5021766	total: 3.6s	remaining: 12.9s
5:	learn: 0.4856543	total: 3.64s	remaining: 10.3s
5:	learn: 0.4894574	total: 4.85s	remaining: 12.9s
6:	learn: 0.4708728	total: 4.08s	remaining: 9.31s
5:	learn: 0.4856793	total: 4.67s	remaining: 13.2s
3:	learn: 0.5261500	total: 2.99s	remaining: 14.2s
5:	learn: 0.4844800	total: 4.29s	remaining: 12.2s
6:	learn: 0.4748154	total: 5.56s	remaining: 11.9s
0:	learn: 0.6358963	total: 713ms	remaining: 15.7s
7:	learn: 0.4600845	total: 4.68s	remaining: 8.77s
4:	learn: 0.5039218	total: 3.8s	remaining: 13.7s
6:	learn: 0.4696249	total: 5.09s	remaining: 11.6s
6:	learn: 0.4712520	total: 5.58s	remaining: 12.8s
0:	learn: 0.6344268	total: 1.11s	remaining: 22.2s
7:	learn: 0.4628703	total: 6.32s	remaining: 11.1s
1:	learn: 0.5916722	total: 1.5s	remaining: 15.8s
8:	learn: 0.4512306	total: 5.21s	remaining: 8.1s
7:	learn: 0.4585697	total: 5.69s	remaining: 10.7s
7:	learn: 0.4605332	total: 6.31s	remaining: 11.8s
0:	learn: 0.6330344	total: 882ms	remaining: 17.6s
9:	learn: 0.4434163	total: 5.8s	remaining: 7.54s
5:	learn: 0.4861833	total: 4.66s	remaining: 13.2s
1:	learn: 0.5883856	total: 1.93s	remaining: 18.3s
8:	learn: 0.4532961	total: 7.08s	remaining: 10.2s
2:	learn: 0.5559819	total: 2.26s	remaining: 15.1s
10:	learn: 0.4378136	total: 6.18s	remaining: 6.74s
8:	learn: 0.4494084	total: 6.35s	remaining: 9.87s
8:	learn: 0.4517224	total: 6.92s	remaining: 10.8s
3:	learn: 0.5288846	total: 2.98s	remaining: 14.1s
1:	learn: 0.5867465	total: 1.85s	remaining: 17.6s
6:	learn: 0.4713426	total: 5.57s	remaining: 12.7s
9:	learn: 0.4461379	total: 8s	remaining: 9.6s
11:	learn: 0.4327181	total: 6.94s	remaining: 6.36s
2:	learn: 0.5522247	total: 3.09s	remaining: 18.5s
9:	learn: 0.4422569	total: 7.12s	remaining: 9.25s
9:	learn: 0.4442003	total: 7.8s	remaining: 10.1s
7:	learn: 0.4608459	total: 6.1s	remaining: 11.4s
4:	learn: 0.5075589	total: 3.64s	remaining: 13.1s
10:	learn: 0.4395294	total: 8.58s	remaining: 8.58s
10:	learn: 0.4366204	total: 7.6s	remaining: 8.29s
12:	learn: 0.4289656	total: 7.53s	remaining: 5.79s
2:	learn: 0.5506386	total: 2.75s	remaining: 16.5s
3:	learn: 0.5237413	total: 3.98s	remaining: 16.9s
10:	learn: 0.4386344	total: 8.48s	remaining: 9.25s
8:	learn: 0.4515858	total: 6.84s	remaining: 10.6s
5:	learn: 0.4894574	total: 4.38s	remaining: 12.4s
11:	learn: 0.4313476	total: 8.12s	remaining: 7.45s
13:	learn: 0.4257749	total: 8.18s	remaining: 5.26s
11:	learn: 0.4340045	total: 9.41s	remaining: 7.85s
3:	learn: 0.5218734	total: 3.58s	remaining: 15.2s
11:	learn: 0.4335373	total: 9.11s	remaining: 8.35s
4:	learn: 0.5017970	total: 4.65s	remaining: 14.9s
12:	learn: 0.4276566	total: 8.81s	remaining: 6.77s
9:	learn: 0.4436350	total: 7.58s	remaining: 9.85s
6:	learn: 0.4748154	total: 5.18s	remaining: 11.8s
14:	learn: 0.4230028	total: 8.88s	remaining: 4.73s
12:	learn: 0.4293924	total: 10.1s	remaining: 6.98s
4:	learn: 0.4994324	total: 4.29s	remaining: 13.7s
12:	learn: 0.4298241	total: 9.79s	remaining: 7.53s
13:	learn: 0.4248066	total: 9.43s	remaining: 6.06s
5:	learn: 0.4827929	total: 5.45s	remaining: 13.6s
15:	learn: 0.4202665	total: 9.51s	remaining: 4.16s
7:	learn: 0.4628703	total: 5.9s	remaining: 11.1s
10:	learn: 0.4381139	total: 8.46s	remaining: 9.23s
13:	learn: 0.4253545	total: 10.9s	remaining: 6.23s
5:	learn: 0.4813581	total: 4.94s	remaining: 12.3s
13:	learn: 0.4271744	total: 10.4s	remaining: 6.68s
14:	learn: 0.4221737	total: 10s	remaining: 5.33s
6:	learn: 0.4697173	total: 6.1s	remaining: 12.2s
11:	learn: 0.4330251	total: 9.08s	remaining: 8.32s
16:	learn: 0.4185146	total: 10.3s	remaining: 3.65s
8:	learn: 0.4532961	total: 6.73s	remaining: 10.5s
14:	learn: 0.4221084	total: 11.6s	remaining: 5.43s
14:	learn: 0.4235656	total: 11.2s	remaining: 5.95s
15:	learn: 0.4193868	total: 10.8s	remaining: 4.72s
12:	learn: 0.4290242	total: 9.55s	remaining: 7.35s
6:	learn: 0.4677947	total: 5.92s	remaining: 11.8s
7:	learn: 0.4574131	total: 6.97s	remaining: 11.3s
15:	learn: 0.4198154	total: 12.4s	remaining: 4.66s
9:	learn: 0.4461379	total: 7.57s	remaining: 9.84s
13:	learn: 0.4256841	total: 10.1s	remaining: 6.48s
17:	learn: 0.4164008	total: 11.3s	remaining: 3.15s
16:	learn: 0.4175573	total: 11.5s	remaining: 4.05s
15:	learn: 0.4207047	total: 12s	remaining: 5.24s
7:	learn: 0.4563248	total: 6.72s	remaining: 10.9s
14:	learn: 0.4227763	total: 10.5s	remaining: 5.62s
8:	learn: 0.4483808	total: 7.89s	remaining: 10.5s
10:	learn: 0.4395294	total: 8.18s	remaining: 8.93s
18:	learn: 0.4141910	total: 12s	remaining: 2.52s
16:	learn: 0.4173313	total: 13.2s	remaining: 3.88s
16:	learn: 0.4190083	total: 12.7s	remaining: 4.48s
15:	learn: 0.4202550	total: 10.9s	remaining: 4.79s
17:	learn: 0.4150931	total: 12.3s	remaining: 3.43s
8:	learn: 0.4461789	total: 7.54s	remaining: 10.1s
16:	learn: 0.4183459	total: 11.6s	remaining: 4.11s
19:	learn: 0.4127386	total: 12.9s	remaining: 1.93s
11:	learn: 0.4340045	total: 9.21s	remaining: 8.45s
17:	learn: 0.4162747	total: 13.5s	remaining: 3.74s
17:	learn: 0.4149399	total: 14.1s	remaining: 3.14s
9:	learn: 0.4416012	total: 9.05s	remaining: 9.96s
18:	learn: 0.4128117	total: 13.2s	remaining: 2.79s
9:	learn: 0.4384327	total: 8.46s	remaining: 9.3s
17:	learn: 0.4158966	total: 12.2s	remaining: 3.4s
18:	learn: 0.4143716	total: 14.1s	remaining: 2.96s
20:	learn: 0.4112902	total: 13.6s	remaining: 1.29s
12:	learn: 0.4293924	total: 9.95s	remaining: 7.65s
18:	learn: 0.4127668	total: 14.9s	remaining: 2.36s
19:	learn: 0.4113793	total: 13.9s	remaining: 2.08s
10:	learn: 0.4354752	total: 10s	remaining: 9.12s
18:	learn: 0.4138350	total: 12.8s	remaining: 2.7s
10:	learn: 0.4324326	total: 9.16s	remaining: 8.33s
19:	learn: 0.4129093	total: 14.6s	remaining: 2.19s
21:	learn: 0.4102003	total: 14.3s	remaining: 649ms
13:	learn: 0.4253545	total: 10.7s	remaining: 6.85s
19:	learn: 0.4113409	total: 15.6s	remaining: 1.56s
20:	learn: 0.4092647	total: 14.6s	remaining: 1.39s
19:	learn: 0.4122843	total: 13.4s	remaining: 2.01s
20:	learn: 0.4114817	total: 15.2s	remaining: 1.44s
11:	learn: 0.4300479	total: 11s	remaining: 8.25s
11:	learn: 0.4268614	total: 10.1s	remaining: 7.56s
22:	learn: 0.4084700	total: 15s	remaining: 0us
20:	learn: 0.4107097	total: 13.9s	remaining: 1.32s
14:	learn: 0.4221084	total: 11.5s	remaining: 6.13s
21:	learn: 0.4104796	total: 15.7s	remaining: 716ms
20:	learn: 0.4096663	total: 16.5s	remaining: 785ms
21:	learn: 0.4073815	total: 15.4s	remaining: 702ms
21:	learn: 0.4090258	total: 14.5s	remaining: 659ms
22:	learn: 0.4091415	total: 16.3s	remaining: 0us
12:	learn: 0.4250861	total: 11.9s	remaining: 7.32s
15:	learn: 0.4198154	total: 12.2s	remaining: 5.33s
12:	learn: 0.4222738	total: 11.2s	remaining: 6.87s
21:	learn: 0.4087722	total: 17.2s	remaining: 0us
22:	learn: 0.4059949	total: 16.2s	remaining: 0us
22:	learn: 0.4077343	total: 14.9s	remaining: 0us
16:	learn: 0.4173313	total: 12.7s	remaining: 4.5s
13:	learn: 0.4212679	total: 12.5s	remaining: 6.25s
13:	learn: 0.4184924	total: 11.8s	remaining: 5.92s
17:	learn: 0.4149399	total: 13.3s	remaining: 3.68s
14:	learn: 0.4190800	total: 13.1s	remaining: 5.23s
14:	learn: 0.4151806	total: 12.4s	remaining: 4.97s
18:	learn: 0.4127668	total: 13.8s	remaining: 2.9s
15:	learn: 0.4156685	total: 13.9s	remaining: 4.36s
19:	learn: 0.4113409	total: 14.3s	remaining: 2.14s
15:	learn: 0.4117304	total: 13.2s	remaining: 4.13s
16:	learn: 0.4132635	total: 14.5s	remaining: 3.41s
20:	learn: 0.4096663	total: 15s	remaining: 1.43s
16:	learn: 0.4091049	total: 14.6s	remaining: 3.42s
21:	learn: 0.4087722	total: 16.4s	remaining: 747ms
17:	learn: 0.4107452	total: 16.3s	remaining: 2.71s
0:	learn: 0.6336168	total: 1.26s	remaining: 25.3s
17:	learn: 0.4074744	total: 15.8s	remaining: 2.64s
0:	learn: 0.6339792	total: 1.22s	remaining: 24.4s
0:	learn: 0.6347365	total: 845ms	remaining: 16.9s
22:	learn: 0.4073936	total: 17.4s	remaining: 0us
18:	learn: 0.4079654	total: 17.2s	remaining: 1.81s
0:	learn: 0.6344268	total: 716ms	remaining: 15s
1:	learn: 0.5868805	total: 2.31s	remaining: 21.9s
18:	learn: 0.4051831	total: 16.8s	remaining: 1.77s
1:	learn: 0.5883909	total: 2.13s	remaining: 20.3s
0:	learn: 0.6330344	total: 1.02s	remaining: 21.5s
1:	learn: 0.5892825	total: 1.85s	remaining: 17.6s
19:	learn: 0.4059461	total: 18.2s	remaining: 910ms
1:	learn: 0.5883856	total: 1.61s	remaining: 16.2s
2:	learn: 0.5521720	total: 2.87s	remaining: 17.2s
19:	learn: 0.4035183	total: 17.8s	remaining: 889ms
2:	learn: 0.5513693	total: 3.4s	remaining: 20.4s
2:	learn: 0.5521365	total: 2.79s	remaining: 16.7s
1:	learn: 0.5867465	total: 2.04s	remaining: 20.4s
20:	learn: 0.4048996	total: 19.1s	remaining: 0us
3:	learn: 0.5230127	total: 3.51s	remaining: 14.9s
2:	learn: 0.5522247	total: 2.54s	remaining: 16.1s
3:	learn: 0.5227429	total: 4.21s	remaining: 17.9s
20:	learn: 0.4005021	total: 18.6s	remaining: 0us
3:	learn: 0.5227181	total: 3.56s	remaining: 15.1s
4:	learn: 0.4997106	total: 4.02s	remaining: 12.9s
2:	learn: 0.5506386	total: 2.97s	remaining: 18.8s
3:	learn: 0.5237413	total: 3.31s	remaining: 14.9s
4:	learn: 0.5004418	total: 4.07s	remaining: 13s
4:	learn: 0.5006470	total: 4.96s	remaining: 15.9s
5:	learn: 0.4809109	total: 4.61s	remaining: 11.5s
3:	learn: 0.5218734	total: 3.88s	remaining: 17.5s
5:	learn: 0.4835125	total: 4.77s	remaining: 11.9s
6:	learn: 0.4674406	total: 5.21s	remaining: 10.4s
5:	learn: 0.4824175	total: 5.67s	remaining: 14.2s
4:	learn: 0.5017970	total: 4.18s	remaining: 14.2s
4:	learn: 0.4994324	total: 4.42s	remaining: 15s
7:	learn: 0.4557107	total: 6.01s	remaining: 9.76s
6:	learn: 0.4692567	total: 6.69s	remaining: 13.4s
5:	learn: 0.4827929	total: 5.19s	remaining: 13.8s
6:	learn: 0.4690357	total: 5.94s	remaining: 11.9s
5:	learn: 0.4813581	total: 5.32s	remaining: 14.2s
8:	learn: 0.4468134	total: 6.68s	remaining: 8.9s
0:	learn: 0.6336168	total: 788ms	remaining: 16.6s
7:	learn: 0.4569852	total: 7.6s	remaining: 12.4s
7:	learn: 0.4578066	total: 6.88s	remaining: 11.2s
6:	learn: 0.4697173	total: 6.14s	remaining: 13.2s
6:	learn: 0.4677947	total: 6.24s	remaining: 13.4s
9:	learn: 0.4392745	total: 7.72s	remaining: 8.49s
1:	learn: 0.5868805	total: 1.7s	remaining: 17s
8:	learn: 0.4483732	total: 7.81s	remaining: 10.4s
8:	learn: 0.4477499	total: 8.76s	remaining: 11.7s
0:	learn: 0.6339792	total: 1s	remaining: 21.1s
2:	learn: 0.5513693	total: 2.39s	remaining: 15.1s
7:	learn: 0.4563248	total: 7.33s	remaining: 12.8s
10:	learn: 0.4335273	total: 8.66s	remaining: 7.87s
7:	learn: 0.4574131	total: 7.64s	remaining: 13.4s
0:	learn: 0.6347365	total: 1.05s	remaining: 22s
3:	learn: 0.5227429	total: 3.3s	remaining: 14.8s
9:	learn: 0.4402780	total: 9.82s	remaining: 10.8s
9:	learn: 0.4402303	total: 9.08s	remaining: 9.99s
8:	learn: 0.4483808	total: 8.53s	remaining: 12.3s
1:	learn: 0.5883909	total: 2.35s	remaining: 23.5s
11:	learn: 0.4279671	total: 9.78s	remaining: 7.34s
4:	learn: 0.5006470	total: 3.72s	remaining: 12.6s
8:	learn: 0.4461789	total: 8.66s	remaining: 12.5s
1:	learn: 0.5892825	total: 2.01s	remaining: 20.1s
10:	learn: 0.4345735	total: 10.6s	remaining: 9.65s
10:	learn: 0.4342987	total: 9.97s	remaining: 9.06s
5:	learn: 0.4824175	total: 4.27s	remaining: 11.4s
2:	learn: 0.5521720	total: 3.22s	remaining: 20.4s
9:	learn: 0.4416012	total: 9.5s	remaining: 11.4s
9:	learn: 0.4384327	total: 9.58s	remaining: 11.5s
12:	learn: 0.4220462	total: 10.9s	remaining: 6.68s
2:	learn: 0.5521365	total: 3.06s	remaining: 19.4s
11:	learn: 0.4291804	total: 11.4s	remaining: 8.52s
6:	learn: 0.4692567	total: 5.01s	remaining: 10.7s
11:	learn: 0.4285322	total: 10.8s	remaining: 8.11s
10:	learn: 0.4354752	total: 10.4s	remaining: 10.4s
3:	learn: 0.5230127	total: 4.14s	remaining: 18.7s
12:	learn: 0.4243468	total: 11.9s	remaining: 7.34s
10:	learn: 0.4324326	total: 10.4s	remaining: 10.4s
13:	learn: 0.4184102	total: 11.7s	remaining: 5.83s
12:	learn: 0.4242295	total: 11.6s	remaining: 7.12s
3:	learn: 0.5227181	total: 4.1s	remaining: 18.5s
7:	learn: 0.4569852	total: 5.91s	remaining: 10.3s
13:	learn: 0.4208968	total: 12.5s	remaining: 6.26s
11:	learn: 0.4300479	total: 11.2s	remaining: 9.3s
4:	learn: 0.4997106	total: 4.94s	remaining: 16.8s
11:	learn: 0.4268614	total: 11.2s	remaining: 9.36s
14:	learn: 0.4155859	total: 12.5s	remaining: 5s
13:	learn: 0.4199046	total: 12.3s	remaining: 6.16s
8:	learn: 0.4477499	total: 6.71s	remaining: 9.69s
4:	learn: 0.5004418	total: 4.95s	remaining: 16.8s
5:	learn: 0.4809109	total: 5.54s	remaining: 14.8s
14:	learn: 0.4171584	total: 13.4s	remaining: 5.34s
12:	learn: 0.4250861	total: 12s	remaining: 8.3s
15:	learn: 0.4120543	total: 13.3s	remaining: 4.17s
12:	learn: 0.4222738	total: 12.2s	remaining: 8.47s
14:	learn: 0.4159981	total: 13.1s	remaining: 5.23s
5:	learn: 0.4835125	total: 5.78s	remaining: 15.4s
9:	learn: 0.4402780	total: 7.63s	remaining: 9.15s
6:	learn: 0.4674406	total: 6.34s	remaining: 13.6s
15:	learn: 0.4138489	total: 14.2s	remaining: 4.42s
13:	learn: 0.4212679	total: 12.9s	remaining: 7.4s
16:	learn: 0.4089261	total: 14.2s	remaining: 3.35s
13:	learn: 0.4184924	total: 13.1s	remaining: 7.5s
15:	learn: 0.4130167	total: 14s	remaining: 4.36s
6:	learn: 0.4690357	total: 6.64s	remaining: 14.2s
7:	learn: 0.4557107	total: 7.16s	remaining: 12.5s
10:	learn: 0.4345735	total: 8.56s	remaining: 8.56s
14:	learn: 0.4190800	total: 13.8s	remaining: 6.46s
16:	learn: 0.4113452	total: 15.4s	remaining: 3.61s
14:	learn: 0.4151806	total: 14s	remaining: 6.54s
7:	learn: 0.4578066	total: 7.36s	remaining: 12.9s
17:	learn: 0.4061326	total: 15.3s	remaining: 2.54s
16:	learn: 0.4102217	total: 14.9s	remaining: 3.5s
8:	learn: 0.4468134	total: 7.96s	remaining: 11.5s
11:	learn: 0.4291804	total: 9.57s	remaining: 7.98s
15:	learn: 0.4156685	total: 14.8s	remaining: 5.54s
17:	learn: 0.4085361	total: 16.4s	remaining: 2.73s
9:	learn: 0.4392745	total: 8.63s	remaining: 10.4s
8:	learn: 0.4483732	total: 8.2s	remaining: 11.8s
15:	learn: 0.4117304	total: 14.9s	remaining: 5.59s
17:	learn: 0.4081004	total: 15.7s	remaining: 2.62s
18:	learn: 0.4036256	total: 16.2s	remaining: 1.71s
10:	learn: 0.4335273	total: 9.12s	remaining: 9.12s
12:	learn: 0.4243468	total: 10.4s	remaining: 7.22s
18:	learn: 0.4061619	total: 17.2s	remaining: 1.81s
16:	learn: 0.4132635	total: 15.7s	remaining: 4.61s
16:	learn: 0.4091049	total: 15.7s	remaining: 4.62s
9:	learn: 0.4402303	total: 9.08s	remaining: 10.9s
18:	learn: 0.4063610	total: 16.7s	remaining: 1.75s
11:	learn: 0.4279671	total: 9.71s	remaining: 8.09s
19:	learn: 0.4019822	total: 17.2s	remaining: 859ms
13:	learn: 0.4208968	total: 11.3s	remaining: 6.44s
19:	learn: 0.4043876	total: 18s	remaining: 899ms
17:	learn: 0.4107452	total: 16.5s	remaining: 3.67s
17:	learn: 0.4074744	total: 16.6s	remaining: 3.69s
12:	learn: 0.4220462	total: 10.4s	remaining: 7.23s
10:	learn: 0.4342987	total: 9.96s	remaining: 9.96s
19:	learn: 0.4046708	total: 17.7s	remaining: 883ms
20:	learn: 0.3995725	total: 18.1s	remaining: 0us
14:	learn: 0.4171584	total: 12.2s	remaining: 5.69s
20:	learn: 0.4028535	total: 18.9s	remaining: 0us
11:	learn: 0.4285322	total: 10.6s	remaining: 8.85s
18:	learn: 0.4079654	total: 17.4s	remaining: 2.75s
18:	learn: 0.4051831	total: 17.4s	remaining: 2.74s
13:	learn: 0.4184102	total: 11.2s	remaining: 6.38s
20:	learn: 0.4032355	total: 18.5s	remaining: 0us
15:	learn: 0.4138489	total: 13s	remaining: 4.89s
12:	learn: 0.4242295	total: 11.3s	remaining: 7.81s
14:	learn: 0.4155859	total: 12.1s	remaining: 5.63s
19:	learn: 0.4035183	total: 18.3s	remaining: 1.83s
19:	learn: 0.4059461	total: 18.4s	remaining: 1.84s
13:	learn: 0.4199046	total: 11.9s	remaining: 6.8s
16:	learn: 0.4113452	total: 13.9s	remaining: 4.08s
15:	learn: 0.4120543	total: 12.9s	remaining: 4.82s
14:	learn: 0.4159981	total: 12.5s	remaining: 5.84s
20:	learn: 0.4005021	total: 19.2s	remaining: 916ms
20:	learn: 0.4048996	total: 19.4s	remaining: 923ms
17:	learn: 0.4085361	total: 14.7s	remaining: 3.26s
15:	learn: 0.4130167	total: 13.2s	remaining: 4.96s
16:	learn: 0.4089261	total: 13.8s	remaining: 4.04s
21:	learn: 0.3984940	total: 20.2s	remaining: 0us
21:	learn: 0.4028870	total: 20.3s	remaining: 0us
18:	learn: 0.4061619	total: 15.6s	remaining: 2.46s
16:	learn: 0.4102217	total: 13.8s	remaining: 4.06s
17:	learn: 0.4061326	total: 14.6s	remaining: 3.25s
19:	learn: 0.4043876	total: 16.3s	remaining: 1.63s
17:	learn: 0.4081004	total: 14.6s	remaining: 3.25s
0:	learn: 0.6344268	total: 770ms	remaining: 16.9s
18:	learn: 0.4036256	total: 15.7s	remaining: 2.47s
20:	learn: 0.4028535	total: 17s	remaining: 810ms
18:	learn: 0.4063610	total: 15.6s	remaining: 2.46s
0:	learn: 0.6330344	total: 738ms	remaining: 16.2s
1:	learn: 0.5883856	total: 1.69s	remaining: 17.7s
21:	learn: 0.4011985	total: 17.8s	remaining: 0us
19:	learn: 0.4019822	total: 16.6s	remaining: 1.66s
0:	learn: 0.6336168	total: 1.03s	remaining: 22.8s
19:	learn: 0.4046708	total: 16.5s	remaining: 1.65s
1:	learn: 0.5867465	total: 1.54s	remaining: 16.2s
2:	learn: 0.5522247	total: 2.66s	remaining: 17.8s
20:	learn: 0.3995725	total: 17.4s	remaining: 829ms
1:	learn: 0.5868805	total: 1.74s	remaining: 18.3s
2:	learn: 0.5506386	total: 2.29s	remaining: 15.2s
20:	learn: 0.4032355	total: 17.4s	remaining: 831ms
3:	learn: 0.5237413	total: 3.73s	remaining: 17.7s
2:	learn: 0.5513693	total: 2.67s	remaining: 17.8s
21:	learn: 0.3980090	total: 18.6s	remaining: 0us
3:	learn: 0.5218734	total: 3.31s	remaining: 15.7s
21:	learn: 0.4013311	total: 18.4s	remaining: 0us
4:	learn: 0.5017970	total: 4.52s	remaining: 16.3s
3:	learn: 0.5227429	total: 3.27s	remaining: 15.5s
0:	learn: 0.6339792	total: 665ms	remaining: 14.6s
0:	learn: 0.6347365	total: 732ms	remaining: 16.1s
4:	learn: 0.4994324	total: 3.98s	remaining: 14.3s
5:	learn: 0.4827929	total: 5.09s	remaining: 14.4s
4:	learn: 0.5006470	total: 3.84s	remaining: 13.8s
1:	learn: 0.5883909	total: 1.18s	remaining: 12.4s
1:	learn: 0.5892825	total: 1.51s	remaining: 15.9s
5:	learn: 0.4813581	total: 4.78s	remaining: 13.5s
6:	learn: 0.4697173	total: 5.7s	remaining: 13s
5:	learn: 0.4824175	total: 4.42s	remaining: 12.5s
2:	learn: 0.5521720	total: 1.79s	remaining: 11.9s
2:	learn: 0.5521365	total: 2s	remaining: 13.3s
7:	learn: 0.4574131	total: 6.29s	remaining: 11.8s
6:	learn: 0.4677947	total: 5.42s	remaining: 12.4s
6:	learn: 0.4692567	total: 5.01s	remaining: 11.5s
3:	learn: 0.5230127	total: 2.36s	remaining: 11.2s
3:	learn: 0.5227181	total: 2.55s	remaining: 12.1s
7:	learn: 0.4563248	total: 6.03s	remaining: 11.3s
7:	learn: 0.4569852	total: 5.55s	remaining: 10.4s
8:	learn: 0.4483808	total: 6.92s	remaining: 10.8s
4:	learn: 0.4997106	total: 2.92s	remaining: 10.5s
4:	learn: 0.5004418	total: 3.03s	remaining: 10.9s
8:	learn: 0.4461789	total: 6.57s	remaining: 10.2s
9:	learn: 0.4416012	total: 7.47s	remaining: 9.71s
8:	learn: 0.4477499	total: 6.15s	remaining: 9.57s
5:	learn: 0.4809109	total: 3.51s	remaining: 9.95s
5:	learn: 0.4835125	total: 3.58s	remaining: 10.2s
10:	learn: 0.4354752	total: 8.01s	remaining: 8.74s
9:	learn: 0.4384327	total: 7.13s	remaining: 9.28s
9:	learn: 0.4402780	total: 6.69s	remaining: 8.69s
6:	learn: 0.4674406	total: 4.11s	remaining: 9.39s
6:	learn: 0.4690357	total: 4.22s	remaining: 9.64s
10:	learn: 0.4345735	total: 7.18s	remaining: 7.83s
11:	learn: 0.4300479	total: 8.56s	remaining: 7.84s
10:	learn: 0.4324326	total: 7.68s	remaining: 8.38s
7:	learn: 0.4557107	total: 4.6s	remaining: 8.63s
7:	learn: 0.4578066	total: 4.81s	remaining: 9.02s
11:	learn: 0.4268614	total: 8.27s	remaining: 7.58s
11:	learn: 0.4291804	total: 7.82s	remaining: 7.17s
8:	learn: 0.4468134	total: 5.16s	remaining: 8.02s
12:	learn: 0.4250861	total: 9.28s	remaining: 7.14s
8:	learn: 0.4483732	total: 5.37s	remaining: 8.36s
12:	learn: 0.4243468	total: 8.37s	remaining: 6.44s
9:	learn: 0.4392745	total: 5.69s	remaining: 7.4s
12:	learn: 0.4222738	total: 8.89s	remaining: 6.84s
13:	learn: 0.4212679	total: 9.77s	remaining: 6.28s
9:	learn: 0.4402303	total: 5.95s	remaining: 7.74s
13:	learn: 0.4208968	total: 8.94s	remaining: 5.75s
10:	learn: 0.4335273	total: 6.28s	remaining: 6.85s
13:	learn: 0.4184924	total: 9.46s	remaining: 6.08s
14:	learn: 0.4190800	total: 10.4s	remaining: 5.52s
10:	learn: 0.4342987	total: 6.48s	remaining: 7.07s
14:	learn: 0.4171584	total: 9.48s	remaining: 5.06s
11:	learn: 0.4279671	total: 6.84s	remaining: 6.27s
14:	learn: 0.4151806	total: 10s	remaining: 5.34s
15:	learn: 0.4156685	total: 10.9s	remaining: 4.78s
11:	learn: 0.4285322	total: 7.12s	remaining: 6.53s
15:	learn: 0.4138489	total: 10s	remaining: 4.38s
12:	learn: 0.4220462	total: 7.38s	remaining: 5.68s
15:	learn: 0.4117304	total: 10.6s	remaining: 4.62s
16:	learn: 0.4132635	total: 11.5s	remaining: 4.06s
12:	learn: 0.4242295	total: 7.6s	remaining: 5.84s
16:	learn: 0.4113452	total: 10.5s	remaining: 3.71s
13:	learn: 0.4184102	total: 7.94s	remaining: 5.1s
16:	learn: 0.4091049	total: 11.2s	remaining: 3.94s
17:	learn: 0.4107452	total: 12.1s	remaining: 3.36s
13:	learn: 0.4199046	total: 8.14s	remaining: 5.23s
17:	learn: 0.4085361	total: 11.1s	remaining: 3.08s
14:	learn: 0.4155859	total: 8.48s	remaining: 4.53s
17:	learn: 0.4074744	total: 11.7s	remaining: 3.26s
18:	learn: 0.4079654	total: 12.7s	remaining: 2.67s
14:	learn: 0.4159981	total: 8.75s	remaining: 4.67s
18:	learn: 0.4061619	total: 11.6s	remaining: 2.45s
15:	learn: 0.4120543	total: 9.04s	remaining: 3.95s
18:	learn: 0.4051831	total: 12.3s	remaining: 2.58s
19:	learn: 0.4059461	total: 13.2s	remaining: 1.98s
15:	learn: 0.4130167	total: 9.27s	remaining: 4.05s
19:	learn: 0.4043876	total: 12.2s	remaining: 1.83s
16:	learn: 0.4089261	total: 9.62s	remaining: 3.4s
20:	learn: 0.4048996	total: 13.7s	remaining: 1.31s
16:	learn: 0.4102217	total: 9.7s	remaining: 3.42s
19:	learn: 0.4035183	total: 12.9s	remaining: 1.94s
20:	learn: 0.4028535	total: 12.8s	remaining: 1.22s
17:	learn: 0.4061326	total: 10.2s	remaining: 2.82s
21:	learn: 0.4028870	total: 14.4s	remaining: 653ms
20:	learn: 0.4005021	total: 13.5s	remaining: 1.29s
17:	learn: 0.4081004	total: 10.3s	remaining: 2.86s
21:	learn: 0.4011985	total: 13.3s	remaining: 607ms
18:	learn: 0.4036256	total: 10.8s	remaining: 2.28s
21:	learn: 0.3984940	total: 14.1s	remaining: 640ms
22:	learn: 0.4017500	total: 15s	remaining: 0us
18:	learn: 0.4063610	total: 10.9s	remaining: 2.3s
22:	learn: 0.4002748	total: 13.8s	remaining: 0us
19:	learn: 0.4019822	total: 11.3s	remaining: 1.7s
19:	learn: 0.4046708	total: 11.4s	remaining: 1.7s
22:	learn: 0.3975005	total: 14.6s	remaining: 0us
20:	learn: 0.3995725	total: 11.7s	remaining: 1.12s
20:	learn: 0.4032355	total: 11.7s	remaining: 1.11s
21:	learn: 0.3980090	total: 12.1s	remaining: 548ms
21:	learn: 0.4013311	total: 12.1s	remaining: 548ms
22:	learn: 0.3959427	total: 12.4s	remaining: 0us
22:	learn: 0.3999677	total: 12.4s	remaining: 0us
0:	learn: 0.6361311	total: 189ms	remaining: 3.79s
1:	learn: 0.5931036	total: 314ms	remaining: 2.98s
2:	learn: 0.5574731	total: 435ms	remaining: 2.61s
3:	learn: 0.5302312	total: 563ms	remaining: 2.39s
4:	learn: 0.5078682	total: 712ms	remaining: 2.28s
5:	learn: 0.4895207	total: 857ms	remaining: 2.14s
6:	learn: 0.4764729	total: 977ms	remaining: 1.95s
7:	learn: 0.4660084	total: 1.1s	remaining: 1.79s
8:	learn: 0.4567142	total: 1.25s	remaining: 1.66s
9:	learn: 0.4489775	total: 1.37s	remaining: 1.51s
10:	learn: 0.4429536	total: 1.5s	remaining: 1.36s
11:	learn: 0.4377223	total: 1.63s	remaining: 1.22s
12:	learn: 0.4331845	total: 1.76s	remaining: 1.08s
13:	learn: 0.4292595	total: 1.88s	remaining: 939ms
14:	learn: 0.4268328	total: 1.99s	remaining: 798ms
15:	learn: 0.4243172	total: 2.12s	remaining: 663ms
16:	learn: 0.4216360	total: 2.25s	remaining: 530ms
17:	learn: 0.4197121	total: 2.42s	remaining: 403ms
18:	learn: 0.4179435	total: 2.56s	remaining: 269ms
19:	learn: 0.4159835	total: 2.68s	remaining: 134ms
20:	learn: 0.4149339	total: 2.82s	remaining: 0us
{'class_weights': [1, 8.61560555289612], 'depth': 6, 'learning_rate': 0.1, 'n_estimators': 21}
================训练集================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98    140446
           1       0.26      0.25      0.25      4178

    accuracy                           0.96    144624
   macro avg       0.62      0.61      0.62    144624
weighted avg       0.96      0.96      0.96    144624

[[137468   2978]
 [  3138   1040]]
AUC=0.6138595252802214
================测试集==============
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     60079
           1       0.24      0.22      0.23      1903

    accuracy                           0.95     61982
   macro avg       0.61      0.60      0.60     61982
weighted avg       0.95      0.95      0.95     61982

[[58753  1326]
 [ 1493   410]]
AUC=0.5966891753323529
===========b_test===================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     20074
           1       0.23      0.21      0.22       587

    accuracy                           0.96     20661
   macro avg       0.60      0.60      0.60     20661
weighted avg       0.96      0.96      0.96     20661

[[19651   423]
 [  462   125]]
AUC=0.5959375778104828
                         features  importance
20                   active_index      11.170
24                    svc_revenue       5.602
28                food_party_size       4.749
61                 CAFE20_RECENCY       4.269
43           rank_preference_food       3.997
11                 IS_SR_KIT_USER       3.921
13                       skr_rate       3.718
21          cafe_tag_p6m_food_qty       3.169
46                   monthly_freq       2.733
75                      p2w_trans       2.657
14                     merch_rate       2.599
6       cafe_tag_is_mop_available       2.559
34             CAFE20_VISIT_MERCH       2.537
7                   is_merch_user       2.529
53                  DD_launch_gap       2.517
31                     SR_KIT_NUM       2.410
47  cafe_tag_p6m_merch_party_size       2.373
67                max_DD_Quantity       2.290
55                        p6m_amt       2.275
23                         DD_rev       2.151
50                      p6m_trans       2.105
72              cafe_tag_p3m_vist       1.797
5         is_LAST_2YEAR_DD_ACTIVE       1.569
33  cafe_tag_p3m_merch_party_size       1.431
70                  MC_launch_gap       1.413
56         cafe_tag_p3m_merch_qty       1.410
68                         MC_rev       1.206
39             CAFE20_RECENCY_APP       1.200
76           CAFE20_RECENCY_SRKIT       1.147
42          cafe_tag_p3m_food_qty       1.130
==========LGB+LR===========
[LightGBM] [Info] Total Bins 1303
[LightGBM] [Info] Number of data points in the train set: 144624, number of used features: 80
[LightGBM] [Info] Start training from score 0.288276
[10]	training's l1: 0.401791	training's l2: 0.196994
[20]	training's l1: 0.393913	training's l2: 0.190093
[30]	training's l1: 0.386766	training's l2: 0.184349
[40]	training's l1: 0.380211	training's l2: 0.179465
[50]	training's l1: 0.374157	training's l2: 0.1753
[60]	training's l1: 0.368545	training's l2: 0.171709
[70]	training's l1: 0.363357	training's l2: 0.16862
[80]	training's l1: 0.358596	training's l2: 0.165967
[90]	training's l1: 0.354151	training's l2: 0.163638
[100]	training's l1: 0.350032	training's l2: 0.161606
================训练集================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98    140446
           1       0.26      0.28      0.27      4178

    accuracy                           0.96    144624
   macro avg       0.62      0.63      0.62    144624
weighted avg       0.96      0.96      0.96    144624

[[137106   3340]
 [  3026   1152]]
AUC=0.6259743160963513
================测试集================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     60079
           1       0.23      0.23      0.23      1903

    accuracy                           0.95     61982
   macro avg       0.60      0.60      0.60     61982
weighted avg       0.95      0.95      0.95     61982

[[58612  1467]
 [ 1460   443]]
AUC=0.6041862406125856
===========b_test===================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     20074
           1       0.24      0.25      0.24       587

    accuracy                           0.96     20661
   macro avg       0.61      0.61      0.61     20661
weighted avg       0.96      0.96      0.96     20661

[[19600   474]
 [  440   147]]
AUC=0.6134066305606224
(144624, 3000)
(0, 0)
================训练集================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98    140446
           1       0.30      0.35      0.33      4178

    accuracy                           0.96    144624
   macro avg       0.64      0.66      0.65    144624
weighted avg       0.96      0.96      0.96    144624

[[137100   3346]
 [  2718   1460]]
AUC=0.6628126970765573
================测试集==============
              precision    recall  f1-score   support

           0       0.98      0.97      0.97     60079
           1       0.21      0.23      0.22      1903

    accuracy                           0.95     61982
   macro avg       0.59      0.60      0.60     61982
weighted avg       0.95      0.95      0.95     61982

[[58437  1642]
 [ 1461   442]]
AUC=0.6024670818559732
===========b_test===================
              precision    recall  f1-score   support

           0       0.98      0.97      0.98     20074
           1       0.20      0.23      0.21       587

    accuracy                           0.95     20661
   macro avg       0.59      0.60      0.60     20661
weighted avg       0.96      0.95      0.95     20661

[[19531   543]
 [  451   136]]
AUC=0.6023183132121542
==========GCfroset==========
[ 2021-04-08 20:41:50,885][cascade_classifier.fit_transform] X_groups_train.shape=[(144624, 80)],y_train.shape=(144624,),X_groups_test.shape=no_test,y_test.shape=no_test
[ 2021-04-08 20:41:50,999][cascade_classifier.fit_transform] group_dims=[80]
[ 2021-04-08 20:41:50,999][cascade_classifier.fit_transform] group_starts=[0]
[ 2021-04-08 20:41:50,999][cascade_classifier.fit_transform] group_ends=[80]
[ 2021-04-08 20:41:50,999][cascade_classifier.fit_transform] X_train.shape=(144624, 80),X_test.shape=(0, 80)
[ 2021-04-08 20:41:51,097][cascade_classifier.fit_transform] [layer=0] look_indexs=[0], X_cur_train.shape=(144624, 80), X_cur_test.shape=(0, 80)
[ 2021-04-08 20:41:51,896][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 2_folds.train_0.predict)=94.98%
[ 2021-04-08 20:41:52,579][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 2_folds.train_1.predict)=95.84%
[ 2021-04-08 20:41:52,583][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 2_folds.train_cv.predict)=95.41%
[ 2021-04-08 20:41:53,335][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 2_folds.train_0.predict)=95.84%
[ 2021-04-08 20:41:54,034][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 2_folds.train_1.predict)=95.83%
[ 2021-04-08 20:41:54,040][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 2_folds.train_cv.predict)=95.83%
[20:41:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[ 2021-04-08 20:41:58,259][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 2_folds.train_0.predict)=94.85%
[20:41:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[ 2021-04-08 20:42:01,638][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 2_folds.train_1.predict)=94.89%
[ 2021-04-08 20:42:01,649][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 2_folds.train_cv.predict)=94.87%
[ 2021-04-08 20:42:01,659][cascade_classifier.calc_accuracy] Accuracy(layer_0 - train.classifier_average)=95.61%
[ 2021-04-08 20:42:01,745][cascade_classifier.fit_transform] [layer=1] look_indexs=[0], X_cur_train.shape=(144624, 86), X_cur_test.shape=(0, 86)
[ 2021-04-08 20:42:02,770][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 2_folds.train_0.predict)=94.69%
[ 2021-04-08 20:42:03,817][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 2_folds.train_1.predict)=94.17%
[ 2021-04-08 20:42:03,823][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 2_folds.train_cv.predict)=94.43%
[ 2021-04-08 20:42:04,634][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 2_folds.train_0.predict)=95.08%
[ 2021-04-08 20:42:05,370][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 2_folds.train_1.predict)=95.22%
[ 2021-04-08 20:42:05,375][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 2_folds.train_cv.predict)=95.15%
[20:42:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[ 2021-04-08 20:42:08,791][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 2_folds.train_0.predict)=94.52%
[20:42:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[ 2021-04-08 20:42:13,187][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 2_folds.train_1.predict)=94.55%
[ 2021-04-08 20:42:13,200][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 2_folds.train_cv.predict)=94.53%
[ 2021-04-08 20:42:13,207][cascade_classifier.calc_accuracy] Accuracy(layer_1 - train.classifier_average)=94.79%
[ 2021-04-08 20:42:13,295][cascade_classifier.fit_transform] [layer=2] look_indexs=[0], X_cur_train.shape=(144624, 86), X_cur_test.shape=(0, 86)
[ 2021-04-08 20:42:14,441][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 2_folds.train_0.predict)=94.13%
[ 2021-04-08 20:42:15,545][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 2_folds.train_1.predict)=93.77%
[ 2021-04-08 20:42:15,555][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 2_folds.train_cv.predict)=93.95%
[ 2021-04-08 20:42:16,375][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 2_folds.train_0.predict)=95.25%
[ 2021-04-08 20:42:17,147][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 2_folds.train_1.predict)=95.07%
[ 2021-04-08 20:42:17,152][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 2_folds.train_cv.predict)=95.16%
[20:42:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[ 2021-04-08 20:42:20,720][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 2_folds.train_0.predict)=94.53%
[20:42:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[ 2021-04-08 20:42:24,378][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 2_folds.train_1.predict)=94.14%
[ 2021-04-08 20:42:24,389][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 2_folds.train_cv.predict)=94.34%
[ 2021-04-08 20:42:24,397][cascade_classifier.calc_accuracy] Accuracy(layer_2 - train.classifier_average)=94.61%
[ 2021-04-08 20:42:24,480][cascade_classifier.fit_transform] [layer=3] look_indexs=[0], X_cur_train.shape=(144624, 86), X_cur_test.shape=(0, 86)
[ 2021-04-08 20:42:25,373][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 2_folds.train_0.predict)=94.26%
[ 2021-04-08 20:42:26,264][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 2_folds.train_1.predict)=94.21%
[ 2021-04-08 20:42:26,271][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 2_folds.train_cv.predict)=94.24%
[ 2021-04-08 20:42:27,026][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 2_folds.train_0.predict)=94.44%
[ 2021-04-08 20:42:27,721][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 2_folds.train_1.predict)=94.50%
[ 2021-04-08 20:42:27,728][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 2_folds.train_cv.predict)=94.47%
[20:42:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[ 2021-04-08 20:42:31,813][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 2_folds.train_0.predict)=94.34%
[20:42:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.
[ 2021-04-08 20:42:38,128][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 2_folds.train_1.predict)=93.95%
[ 2021-04-08 20:42:38,138][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 2_folds.train_cv.predict)=94.15%
[ 2021-04-08 20:42:38,145][cascade_classifier.calc_accuracy] Accuracy(layer_3 - train.classifier_average)=94.37%
[ 2021-04-08 20:42:38,146][cascade_classifier.fit_transform] [Result][Optimal Level Detected] opt_layer_num=1, accuracy_train=95.61%, accuracy_test=0.00%
[ 2021-04-08 20:42:38,185][cascade_classifier.transform] X_groups_test.shape=[(144624, 80)]
================训练集================
[ 2021-04-08 20:42:38,252][cascade_classifier.transform] group_dims=[80]
[ 2021-04-08 20:42:38,252][cascade_classifier.transform] X_test.shape=(144624, 80)
[ 2021-04-08 20:42:38,312][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(144624, 80)
              precision    recall  f1-score   support

           0       0.98      0.98      0.98    140446
           1       0.27      0.26      0.27      4178

    accuracy                           0.96    144624
   macro avg       0.62      0.62      0.62    144624
weighted avg       0.96      0.96      0.96    144624

[[137413   3033]
 [  3076   1102]]
AUC=0.6210835385817024
[ 2021-04-08 20:42:43,723][cascade_classifier.transform] X_groups_test.shape=[(61982, 80)]
================测试集================
[ 2021-04-08 20:42:43,756][cascade_classifier.transform] group_dims=[80]
[ 2021-04-08 20:42:43,756][cascade_classifier.transform] X_test.shape=(61982, 80)
[ 2021-04-08 20:42:43,779][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(61982, 80)
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     60079
           1       0.24      0.23      0.23      1903

    accuracy                           0.95     61982
   macro avg       0.61      0.60      0.61     61982
weighted avg       0.95      0.95      0.95     61982

[[58725  1354]
 [ 1471   432]]
AUC=0.6022364956380738
===========b_test===================
[ 2021-04-08 20:42:47,393][cascade_classifier.transform] X_groups_test.shape=[(20661, 80)]
[ 2021-04-08 20:42:47,402][cascade_classifier.transform] group_dims=[80]
[ 2021-04-08 20:42:47,403][cascade_classifier.transform] X_test.shape=(20661, 80)
[ 2021-04-08 20:42:47,410][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(20661, 80)
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     20074
           1       0.24      0.24      0.24       587

    accuracy                           0.96     20661
   macro avg       0.61      0.61      0.61     20661
weighted avg       0.96      0.96      0.96     20661

[[19636   438]
 [  449   138]]
AUC=0.6066372140287071
==============start_stacking================
rf now score is: [0.06032904056829179]
rf now score is: [0.06032904056829179, 0.06130292839379897]
rf now score is: [0.06032904056829179, 0.06130292839379897, 0.06116350138000929]
rf now score is: [0.06032904056829179, 0.06130292839379897, 0.06116350138000929, 0.06201748669894418]
rf now score is: [0.06032904056829179, 0.06130292839379897, 0.06116350138000929, 0.06201748669894418, 0.062056656681390925]
rf_score_list: [0.06032904056829179, 0.06130292839379897, 0.06116350138000929, 0.06201748669894418, 0.062056656681390925]
rf_score_mean: 0.06137392274448703
==========LGB===========
[LightGBM] [Info] Total Bins 1343
[LightGBM] [Info] Number of data points in the train set: 144624, number of used features: 82
[LightGBM] [Info] Start training from score 0.256805
[10]	training's l1: 0.373476	training's l2: 0.182995
[20]	training's l1: 0.36601	training's l2: 0.176505
[30]	training's l1: 0.359151	training's l2: 0.171069
[40]	training's l1: 0.352911	training's l2: 0.166536
[50]	training's l1: 0.347218	training's l2: 0.162713
[60]	training's l1: 0.342008	training's l2: 0.159493
[70]	training's l1: 0.337224	training's l2: 0.156753
[80]	training's l1: 0.332881	training's l2: 0.154456
[90]	training's l1: 0.328893	training's l2: 0.152469
[100]	training's l1: 0.325226	training's l2: 0.150776
================训练集================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98    140446
           1       0.26      0.25      0.25      4178

    accuracy                           0.96    144624
   macro avg       0.62      0.61      0.61    144624
weighted avg       0.96      0.96      0.96    144624

[[137427   3019]
 [  3141   1037]]
AUC=0.6133545382508342
================测试集================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     60079
           1       0.24      0.22      0.23      1903

    accuracy                           0.95     61982
   macro avg       0.61      0.60      0.60     61982
weighted avg       0.95      0.95      0.95     61982

[[58717  1362]
 [ 1481   422]]
AUC=0.5995424862606676
===========b_test===================
              precision    recall  f1-score   support

           0       0.98      0.98      0.98     20074
           1       0.23      0.23      0.23       587

    accuracy                           0.96     20661
   macro avg       0.60      0.60      0.60     20661
weighted avg       0.96      0.96      0.96     20661

[[19610   464]
 [  451   136]]
AUC=0.6042860326502333
================Importance================
                     features  importance
81                 stacking_1         225
80                 stacking_0         201
5     is_LAST_2YEAR_DD_ACTIVE         149
24                svc_revenue         137
19                   citytier         118
1               CAFE20_region         118
4                   level_use         116
28            food_party_size         108
39         CAFE20_RECENCY_APP         105
21      cafe_tag_p6m_food_qty          94
58                 MC_end_gap          84
6   cafe_tag_is_mop_available          83
13                   skr_rate          81
23                     DD_rev          70
60           CAFE20_VISIT_BEV          62
63     cafe_tag_p6m_merch_qty          58
38                 CAFE20_age          56
74         CAFE20_VISIT_SRKIT          51
42      cafe_tag_p3m_food_qty          48
17          p6m_avg_order_amt          44
36           CAFE20_VISIT_APP          44
43       rank_preference_food          41
11             IS_SR_KIT_USER          39
72          cafe_tag_p3m_vist          38
50                  p6m_trans          37
53              DD_launch_gap          34
34         CAFE20_VISIT_MERCH          32
52                 DD_end_gap          31
22                  total_amt          30
40    CAFE20_RECENCY_bev_food          26

Process finished with exit code 0
